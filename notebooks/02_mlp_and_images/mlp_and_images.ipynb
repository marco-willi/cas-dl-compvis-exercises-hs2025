{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer-Perceptron and Images\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Train an MLP and understand how it works on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9856bbf98ea570f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Google Colab Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Detect Colab\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "print(f\"In Colab: {IN_COLAB}\")\n",
    "\n",
    "# Show prominent message if in Colab\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        from IPython.display import Markdown, display\n",
    "\n",
    "        display(\n",
    "            Markdown(\n",
    "                \"\"\"\n",
    "> ðŸ’¾ **Optionally:**  \n",
    "> Save this notebook to your **personal Google Drive** to persist any changes.\n",
    ">\n",
    "> *Go to `File â–¸ Save a copy in Drive` before editing.*\n",
    "            \"\"\"\n",
    "            )\n",
    "        )\n",
    "    except Exception:\n",
    "        print(\n",
    "            \"\\nðŸ’¾ Optionally: Save the notebook to your personal Google Drive to persist changes.\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mount google drive to store data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Data Path\n",
    "\n",
    "**Modify the following paths if necessary.**\n",
    "\n",
    "That is where your data will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if IN_COLAB:\n",
    "    DATA_PATH = Path(\"/content/drive/MyDrive/cas-dl-module-compvis-part1\")\n",
    "else:\n",
    "    DATA_PATH = Path(\"../../data\")\n",
    "assert DATA_PATH.exists(), f\"PATH: {DATA_PATH} does not exist.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Lectures Package\n",
    "\n",
    "Install `dl_cv_lectures` package with all necessary dependencies.\n",
    "\n",
    "This package provides the environment of the exercises-repository, as well as helper- and utils modules: [Link](https://github.com/marco-willi/cas-dl-compvis-exercises-hs2025)\n",
    "\n",
    "The following code installs the package from a local repository (if available), otherwise it installs it from the exercise repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "\n",
    "def ensure_dl_cv_lectures():\n",
    "    \"\"\"Ensure dl_cv_lectures is installed (local or from GitHub).\"\"\"\n",
    "    try:\n",
    "        import dl_cv_lectures\n",
    "\n",
    "        console.print(\n",
    "            \"[bold green]âœ… dl_cv_lectures installed â€” all good![/bold green]\"\n",
    "        )\n",
    "        return\n",
    "    except ImportError:\n",
    "        console.print(\"[bold yellow]âš ï¸ dl_cv_lectures not found.[/bold yellow]\")\n",
    "    repo_path = Path(\"/workspace/pyproject.toml\")\n",
    "    if repo_path.exists():\n",
    "        console.print(\"[cyan]ðŸ“¦ Installing from local repository...[/cyan]\")\n",
    "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \"/workspace\"]\n",
    "    else:\n",
    "        console.print(\"[cyan]ðŸŒ Installing from GitHub repository...[/cyan]\")\n",
    "        cmd = [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"git+https://github.com/marco-willi/cas-dl-compvis-exercises-hs2025\",\n",
    "        ]\n",
    "    try:\n",
    "        subprocess.run(cmd, check=True)\n",
    "        console.print(\"[bold green]âœ… Installation successful![/bold green]\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        console.print(f\"[bold red]âŒ Installation failed ({e}).[/bold red]\")\n",
    "\n",
    "\n",
    "ensure_dl_cv_lectures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries\n",
    "\n",
    "Load all libraries and packages used in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchinfo\n",
    "import torchshow as ts\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.v2 import functional as TF\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from dl_cv_lectures import visualize\n",
    "from dl_cv_lectures.classification import train_one_epoch\n",
    "from dl_cv_lectures.data import pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a default device for your computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Define Data, Loaders, and MLP\n",
    "\n",
    "Quickly get data and train an MLP.\n",
    "\n",
    "We use some pre-defined functionality from the `dl_cv_lectures` package. Feel free to substitute with your own functionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We use a dataset that consists of inspection images of microscopic components. Some of them are faulty (label=1) and need to be identified and sorted out.\n",
    "\n",
    "Lets load the data and take a look at it.\n",
    "\n",
    "First we create a [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = pattern.PatternDataset(\n",
    "    num_samples=100, image_side_length=16, seed=123, max_errors=3, max_x_y_shift=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = ds_train[0]\n",
    "print(image.size)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collect a few samples, visualize them and try to understand how label and images are related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_and_labels_from_ds(\n",
    "    ds: torch.utils.data.Dataset, num_images_to_fetch: int = 16\n",
    ") -> list[torch.Tensor]:\n",
    "    \"\"\"Fetch first n images from a torch.utils.data.Dataset with (image, label) signature.\"\"\"\n",
    "    # for each image: convert it to (C x H x W) format and scale to 0-1\n",
    "    images = [\n",
    "        TF.to_image(ds[i][0]).to(torch.float32) / 255.0\n",
    "        for i in range(0, num_images_to_fetch)\n",
    "    ]\n",
    "    labels = [ds[i][1] for i in range(0, num_images_to_fetch)]\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = get_images_and_labels_from_ds(ds_train, num_images_to_fetch=16)\n",
    "\n",
    "fig, ax = visualize.plot_square_collage_with_captions(\n",
    "    images, [f\"Label: {label}\" for label in labels], global_normalize=True\n",
    ")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: When is a pattern label=0, when is it label=1?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "The pattern is labeled as **label=0** when it's **without errors** (a perfect grid / line pattern), and **label=1** when it contains **errors** (off-line pixels, up to `max_errors=3` deviations from the perfect pattern). The dataset simulates a quality inspection task where label=1 indicates a defective/faulty component.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Architecture\n",
    "\n",
    "Next, we define an MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"A Multi-Layer Perceptron (MLP) model for classification.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hidden: int,\n",
    "        num_classes: int,\n",
    "        input_size: tuple[int, int, int] = (1, 28, 28),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_hidden (int): Number of neurons in the hidden layer.\n",
    "            num_classes (int): Number of output classes for classification.\n",
    "            input_size tuple[int, int, int]: The dimensions of the input image.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Flatten the input image into a 1D tensor\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Hidden layer: fully connected layer from input_size to num_hidden neurons.\n",
    "        self.hidden = nn.Linear(\n",
    "            in_features=input_size[0] * input_size[1] * input_size[2],\n",
    "            out_features=num_hidden,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        # Output layer: fully connected layer from num_hidden neurons to num_classes outputs.\n",
    "        self.output = nn.Linear(in_features=num_hidden, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the MLP model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits (before softmax).\n",
    "        \"\"\"\n",
    "        # Flatten the input tensor into (batch_size, input_size[0] * input_size[1])\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Apply the hidden layer (linear transformation)\n",
    "        x = self.hidden(x)\n",
    "\n",
    "        # Apply ReLU activation function to introduce non-linearity\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Apply the output layer (linear transformation) to get the logits\n",
    "        x = self.output(x)\n",
    "\n",
    "        # Return the output logits (not yet passed through softmax)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets initialize the model and inspect it using `torchinfo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "net = MLP(num_hidden=16, num_classes=2, input_size=(1, 16, 16))\n",
    "print(net)\n",
    "print(torchinfo.summary(net, input_size=(1, 1, 16, 16)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-1, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the training dataset and dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "ds_train = pattern.PatternDataset(\n",
    "    num_samples=100000,\n",
    "    seed=123,\n",
    "    max_errors=3,\n",
    "    max_x_y_shift=0,\n",
    "    transform=image_transforms,\n",
    ")\n",
    "\n",
    "dl_train = torch.utils.data.DataLoader(\n",
    "    ds_train, batch_size=128, shuffle=True, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "That's it! We are ready to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs = 3\n",
    "for epoch in range(0, total_epochs):\n",
    "    print(f\"Starting Epoch: {epoch + 1} / {total_epochs}\")\n",
    "    train_one_epoch(dl_train, net, optimizer, loss_fn, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Analyse MLP properties\n",
    "\n",
    "Now it is getting interesting! \n",
    "\n",
    "We want to inspect the weights of the MLP.\n",
    "\n",
    "We can access the layers of our model by the attributes that we defined.\n",
    "\n",
    "For example we can access the attribute `hidden` of our `MLP`  object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = net.hidden.weight\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight matrix $\\mathbf{W}$ is multiplied with an image $\\mathbf{x}$ to produce the layer activations $\\mathbf{a}$ with: $\\mathbf{a} = \\mathbf{x} \\mathbf{W}^T + \\mathbf{b}$\n",
    "\n",
    "We can see that the hidden layer has 16 neurons, each of which is connected to all input neurons.\n",
    "\n",
    "Each row in $\\mathbf{W}$ can be visualized as an image. We need to reshape it accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What do you expect to see when visualizing the weights?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "You should expect to see **pattern-like structures** in the weight visualizations. Each neuron in the hidden layer learns to detect specific features or patterns in the input images. Since the task involves detecting errors in a grid pattern, the weights should resemble:\n",
    "- Grid-like structures\n",
    "- Edge detectors\n",
    "- Pattern detectors for the expected/unexpected pixel positions\n",
    "\n",
    "The weights essentially represent what each neuron is \"looking for\" in the input image. Neurons that activate strongly for specific patterns will have weights that visually resemble those patterns.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_weights(\n",
    "    weights: torch.Tensor, figsize: tuple[int, int] = (12, 12), scale_each: bool = True\n",
    "):\n",
    "    num_neurons = weights.shape[0]\n",
    "    dim = weights.shape[1]\n",
    "\n",
    "    side_length = int(math.sqrt(dim))\n",
    "\n",
    "    weights = weights.reshape(num_neurons, 1, side_length, side_length)\n",
    "\n",
    "    nrow = int(math.sqrt(num_neurons))\n",
    "    image_grid = TF.to_pil_image(\n",
    "        make_grid(weights, nrow=nrow, normalize=True, scale_each=scale_each)\n",
    "    )\n",
    "\n",
    "    _, ax = plt.subplots(figsize=figsize)\n",
    "    _ = ax.imshow(image_grid, cmap=\"Greys_r\")\n",
    "    _ = ax.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_weights(weights, scale_each=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How do you interpret the weights? How does it match with your expectations?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "The weights show learned **feature detectors** that each neuron uses to identify patterns in the input. You should observe:\n",
    "\n",
    "- **Grid-like patterns**: Some neurons have learned to detect the regular grid structure\n",
    "- **Local patterns**: Weights highlight specific pixel configurations that distinguish label=0 from label=1\n",
    "- **Complementary features**: Different neurons capture different aspects of the pattern (some may detect presence of grid, others detect absence/errors)\n",
    "\n",
    "This matches expectations because the MLP learns to decompose the classification task into detecting multiple local features. Each hidden neuron specializes in recognizing a specific pattern aspect, and the output layer combines these features to make the final classification decision.\n",
    "\n",
    "The visualization reveals that **MLPs learn interpretable features** when trained on structured data like grid patterns.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What happens if we reduce the hidden layer to just 2 neurons?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "With only **2 neurons in the hidden layer**, the model has severely limited capacity:\n",
    "\n",
    "- **Reduced representational power**: Only 2 feature detectors to capture all relevant patterns\n",
    "- **Simpler learned features**: Each neuron must capture more general/coarse features since there are fewer neurons\n",
    "- **Potentially lower accuracy**: The model may struggle to distinguish all cases, especially edge cases\n",
    "- **More visible/interpretable weights**: With just 2 neurons, it's easier to see what each one has learned\n",
    "\n",
    "The weights will show the **most important/discriminative features** that the model found for separating the two classes. The model is forced to learn a very compact representation, essentially finding the 2 most critical patterns needed for classification.\n",
    "\n",
    "This demonstrates the importance of **model capacity** - too few neurons leads to underfitting, while too many might overfit.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "net = MLP(num_hidden=2, num_classes=2, input_size=(1, 16, 16))\n",
    "dl_train = torch.utils.data.DataLoader(\n",
    "    ds_train, batch_size=128, shuffle=True, num_workers=4\n",
    ")\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-1, weight_decay=1e-3)\n",
    "total_epochs = 5\n",
    "for epoch in range(0, total_epochs):\n",
    "    print(f\"Starting Epoch: {epoch + 1} / {total_epochs}\")\n",
    "    train_one_epoch(dl_train, net, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_weights(net.hidden.weight, scale_each=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) What if?\n",
    "\n",
    "What if we make it a bit more difficult and let the pattern randomly shift spatially?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = pattern.PatternDataset(\n",
    "    num_samples=100000, seed=123, max_errors=3, max_x_y_shift=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = get_images_and_labels_from_ds(ds_train, num_images_to_fetch=16)\n",
    "\n",
    "fig, ax = visualize.plot_square_collage_with_captions(\n",
    "    images, [f\"Label: {label}\" for label in labels], global_normalize=True\n",
    ")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Is this a more difficult problem? How will the weights differ from the simpler case?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Yes, this is a significantly more difficult problem!**\n",
    "\n",
    "**Why it's harder:**\n",
    "- **Spatial variation**: Patterns can now appear at different positions (up to Â±1 pixel shift)\n",
    "- **Loss of spatial structure**: The MLP cannot exploit spatial relationships - it treats each pixel independently\n",
    "- **More data needed**: The model needs to learn that the same pattern at different positions has the same meaning\n",
    "\n",
    "**How weights will differ:**\n",
    "- **Less crisp patterns**: Weights become more blurred/diffuse since they need to account for multiple positions\n",
    "- **More redundancy**: Multiple neurons may learn similar but shifted versions of the same pattern\n",
    "- **Harder to interpret**: The clear grid structures may become less visible as neurons try to be position-invariant\n",
    "\n",
    "This highlights a fundamental **limitation of MLPs for image data**: they lack **translation invariance**. This is why CNNs with their sliding convolutional kernels are much better suited for image tasks - they naturally handle spatial shifts.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "net = MLP(num_hidden=16, num_classes=2, input_size=(1, 16, 16))\n",
    "ds_train = pattern.PatternDataset(\n",
    "    num_samples=100000,\n",
    "    seed=123,\n",
    "    max_errors=3,\n",
    "    max_x_y_shift=1,\n",
    "    transform=image_transforms,\n",
    ")\n",
    "dl_train = torch.utils.data.DataLoader(\n",
    "    ds_train, batch_size=128, shuffle=True, num_workers=4\n",
    ")\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-1, weight_decay=1e-3)\n",
    "total_epochs = 5\n",
    "for epoch in range(0, total_epochs):\n",
    "    print(f\"Starting Epoch: {epoch + 1} / {total_epochs}\")\n",
    "    train_one_epoch(dl_train, net, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_weights(net.hidden.weight, scale_each=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is already a bit more difficult to interpret!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Let's try somethig more crazy!\n",
    "\n",
    "We extend the images and place each pattern in a random quarant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_cv_lectures.transform import RandomQuadrantPad\n",
    "\n",
    "torch.manual_seed(123)\n",
    "ds_train = pattern.PatternDataset(\n",
    "    num_samples=100000,\n",
    "    seed=123,\n",
    "    max_errors=3,\n",
    "    max_x_y_shift=0,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            RandomQuadrantPad(),\n",
    "            transforms.Lambda(lambda x: TF.to_pil_image(x)),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = get_images_and_labels_from_ds(ds_train, num_images_to_fetch=16)\n",
    "\n",
    "fig, ax = visualize.plot_square_collage_with_captions(\n",
    "    images, [f\"Label: {label}\" for label in labels], global_normalize=True\n",
    ")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What do you think happens here?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "The `RandomQuadrantPad()` transformation is **randomly placing the 16Ã—16 pattern into one of the four quadrants** of a larger 32Ã—32 image:\n",
    "\n",
    "- **Input**: 16Ã—16 pattern image\n",
    "- **Output**: 32Ã—32 image with the pattern in top-left, top-right, bottom-left, or bottom-right quadrant\n",
    "- **Other quadrants**: Filled with zeros (black)\n",
    "\n",
    "**Purpose:**\n",
    "This creates **extreme spatial variation** to test the MLP's ability to handle position-dependent features. The pattern can now appear in 4 completely different locations, making the task much harder for an MLP.\n",
    "\n",
    "**Impact on MLP:**\n",
    "- The MLP input size increases from 256 (16Ã—16) to 1024 (32Ã—32) pixels\n",
    "- Many more weights to learn\n",
    "- The model must learn 4 different \"versions\" of the same pattern at different positions\n",
    "- This further demonstrates why **MLPs are not well-suited for spatially-varying image data**\n",
    "\n",
    "This sets up a comparison with CNNs, which handle such spatial variations much more naturally through their translation-invariant convolutions.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "net = MLP(num_hidden=16, num_classes=2, input_size=(1, 16 * 2, 16 * 2))\n",
    "ds_train = pattern.PatternDataset(\n",
    "    num_samples=100000,\n",
    "    seed=123,\n",
    "    max_errors=3,\n",
    "    max_x_y_shift=0,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            RandomQuadrantPad(),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "dl_train = torch.utils.data.DataLoader(\n",
    "    ds_train, batch_size=128, shuffle=True, num_workers=4\n",
    ")\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-1, weight_decay=1e-3)\n",
    "total_epochs = 5\n",
    "for epoch in range(0, total_epochs):\n",
    "    print(f\"Starting Epoch: {epoch + 1} / {total_epochs}\")\n",
    "    train_one_epoch(dl_train, net, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_weights(net.hidden.weight, scale_each=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) What happens if we increase the positional uncertainty?\n",
    "\n",
    "Let's randomly shift the pattern by up to 3 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "ds_train = pattern.PatternDataset(\n",
    "    num_samples=100000,\n",
    "    seed=123,\n",
    "    max_errors=3,\n",
    "    max_x_y_shift=3,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            RandomQuadrantPad(),\n",
    "            transforms.Lambda(lambda x: TF.to_pil_image(x)),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "images, labels = get_images_and_labels_from_ds(ds_train, num_images_to_fetch=16)\n",
    "\n",
    "fig, ax = visualize.plot_square_collage_with_captions(\n",
    "    images, [f\"Label: {label}\" for label in labels], global_normalize=True\n",
    ")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We switch to the Adam optimizer which is often much faster and needs less care tuning learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "net = MLP(num_hidden=16, num_classes=2, input_size=(1, 16 * 2, 16 * 2))\n",
    "ds_train = pattern.PatternDataset(\n",
    "    num_samples=100000,\n",
    "    seed=123,\n",
    "    max_errors=3,\n",
    "    max_x_y_shift=3,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            RandomQuadrantPad(),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "dl_train = torch.utils.data.DataLoader(\n",
    "    ds_train, batch_size=128, shuffle=True, num_workers=4\n",
    ")\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "total_epochs = 5\n",
    "for epoch in range(0, total_epochs):\n",
    "    print(f\"Starting Epoch: {epoch + 1} / {total_epochs}\")\n",
    "    train_one_epoch(dl_train, net, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What do you observe?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "With the increased positional uncertainty (up to Â±3 pixel shift AND random quadrant placement), you should observe:\n",
    "\n",
    "**Weight visualization insights:**\n",
    "- **Very blurred/diffuse patterns**: Weights no longer show clear grid structures\n",
    "- **Spread-out activations**: Each neuron tries to capture patterns across a wider spatial area\n",
    "- **Loss of interpretability**: Hard to see what each neuron has learned\n",
    "- **Potentially four repeated patterns**: Some neurons might learn similar features positioned for different quadrants\n",
    "\n",
    "**Training characteristics:**\n",
    "- **Slower convergence**: Model takes longer to learn due to increased complexity\n",
    "- **Potentially lower accuracy**: Even with more neurons, the MLP struggles with this level of spatial variation\n",
    "- **More training data needed**: The 100,000 samples help, but the problem is fundamentally challenging for MLPs\n",
    "\n",
    "**Key insight:**\n",
    "This demonstrates the **fundamental limitation of MLPs for images with spatial variation**. The fully-connected architecture cannot efficiently learn position-invariant features. This motivates the need for **Convolutional Neural Networks (CNNs)**, which are designed to handle spatial variations through:\n",
    "- Local receptive fields\n",
    "- Weight sharing\n",
    "- Translation equivariance\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_weights(net.hidden.weight, scale_each=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) A Peek Ahead: What happens if we choose a CNN?\n",
    "\n",
    "Finally! Let's ditch the MLP.\n",
    "\n",
    "The following CNN is deeper but has ~30 times fewer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, kernel_size: int, num_classes=10, num_filters=16):\n",
    "        super().__init__()\n",
    "\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        # First convolutional layer: 1 input channel (grayscale), 16 output channels, 3x3 kernel\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        # Second convolutional layer: 16 input channels, 32 output channels, 3x3 kernel\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=num_filters,\n",
    "            out_channels=num_filters * 2,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        # Third convolutional layer: 32 input channels, 64 output channels, 3x3 kernel\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=num_filters * 2,\n",
    "            out_channels=num_filters * 4,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Global Average Pooling\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))  # Output size is 1x1 per feature map\n",
    "\n",
    "        # Final output layer\n",
    "        self.fc = nn.Linear(in_features=num_filters * 4, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First conv layer with ReLU\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # Second conv layer with ReLU and max pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Third conv layer with ReLU and max pooling\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        # Global Average Pooling (GAP)\n",
    "        x = self.gap(x)  # Shape will be (batch_size, 64, 1, 1)\n",
    "\n",
    "        # Flatten the GAP output to feed into the fully connected layer\n",
    "        x = torch.flatten(x, 1)  # Shape (batch_size, 64)\n",
    "\n",
    "        # Final fully connected layer (acts as the output layer)\n",
    "        x = self.fc(x)  # Shape (batch_size, num_classes)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SmallCNN(kernel_size=5, num_classes=2, num_filters=2)\n",
    "print(torchinfo.summary(net, input_size=(1, 1, 16 * 2, 16 * 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "net = SmallCNN(kernel_size=5, num_classes=2, num_filters=2)\n",
    "ds_train = pattern.PatternDataset(\n",
    "    num_samples=100000,\n",
    "    seed=123,\n",
    "    max_errors=3,\n",
    "    max_x_y_shift=3,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            RandomQuadrantPad(),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "dl_train = torch.utils.data.DataLoader(\n",
    "    ds_train, batch_size=128, shuffle=True, num_workers=6\n",
    ")\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "total_epochs = 3\n",
    "for epoch in range(0, total_epochs):\n",
    "    print(f\"Starting Epoch: {epoch + 1} / {total_epochs}\")\n",
    "    train_one_epoch(dl_train, net, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN uses only two filters in the first layer, which we can visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.show(net.conv1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize what these filters actually detect when applied to input images. We'll look at the activations (feature maps) produced by each filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of 16 sample images\n",
    "ds_sample = pattern.PatternDataset(\n",
    "    num_samples=16,\n",
    "    seed=123,\n",
    "    max_errors=3,\n",
    "    max_x_y_shift=3,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            RandomQuadrantPad(),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "sample_images = torch.stack([ds_sample[i][0] for i in range(16)])\n",
    "sample_labels = [ds_sample[i][1] for i in range(16)]\n",
    "\n",
    "# Display the sample images\n",
    "print(f\"Sample images shape: {sample_images.shape}\")\n",
    "ts.show(sample_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Activations from Filter 1\n",
    "\n",
    "Apply the first convolutional layer and extract the activations from the first filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply conv1 to get all feature maps\n",
    "with torch.no_grad():\n",
    "    conv1_output = net.conv1(sample_images)  # Shape: (16, 2, 32, 32)\n",
    "    conv1_relu = F.relu(conv1_output)\n",
    "\n",
    "# Extract activations from first filter (filter 0)\n",
    "filter1_activations = conv1_relu[:, 0:1, :, :]  # Shape: (16, 1, 32, 32)\n",
    "\n",
    "print(f\"Filter 1 activations shape: {filter1_activations.shape}\")\n",
    "ts.show(filter1_activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Activations from Filter 2\n",
    "\n",
    "Now let's see what the second filter detects in the same images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract activations from second filter (filter 1)\n",
    "filter2_activations = conv1_relu[:, 1:2, :, :]  # Shape: (16, 1, 32, 32)\n",
    "\n",
    "print(f\"Filter 2 activations shape: {filter2_activations.shape}\")\n",
    "ts.show(filter2_activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: Original Images, Filter 1, and Filter 2\n",
    "\n",
    "Let's create a side-by-side comparison to see what each filter detects in a few example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison for the first 4 images\n",
    "num_to_show = 8\n",
    "\n",
    "fig, axes = plt.subplots(num_to_show, 3, figsize=(12, num_to_show * 3))\n",
    "\n",
    "for i in range(num_to_show):\n",
    "    # Original image\n",
    "    _ = axes[i, 0].imshow(sample_images[i, 0].cpu(), cmap=\"gray\")\n",
    "    _ = axes[i, 0].set_title(f\"Original Image {i + 1}\\nLabel: {sample_labels[i]}\")\n",
    "    _ = axes[i, 0].axis(\"off\")\n",
    "\n",
    "    # Filter 1 activation\n",
    "    _ = axes[i, 1].imshow(filter1_activations[i, 0].cpu(), cmap=\"viridis\")\n",
    "    _ = axes[i, 1].set_title(\"Filter 1 Activation\")\n",
    "    _ = axes[i, 1].axis(\"off\")\n",
    "\n",
    "    # Filter 2 activation\n",
    "    _ = axes[i, 2].imshow(filter2_activations[i, 0].cpu(), cmap=\"viridis\")\n",
    "    _ = axes[i, 2].set_title(\"Filter 2 Activation\")\n",
    "    _ = axes[i, 2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation:\")\n",
    "print(\n",
    "    \"- Bright regions (yellow) in the activation maps show where the filter strongly responds\"\n",
    ")\n",
    "print(\"- Dark regions (purple) show where the filter doesn't respond much\")\n",
    "print(\"- Each filter learns to detect different patterns in the input images\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
