{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# PyTorch and Images\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Know / Refresh the different components of a PyTorch ML Pipeline: Dataset, Data-Transforms, Data-Loader, Loss-Function, Logging & Metrics, Trainer, Evaluation\n",
    "- Be able to read, visualize and inspect images.\n",
    "- Train a neural network (MLP) on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Detect Colab\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "print(f\"In Colab: {IN_COLAB}\")\n",
    "\n",
    "# Show prominent message if in Colab\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        from IPython.display import Markdown, display\n",
    "\n",
    "        display(\n",
    "            Markdown(\n",
    "                \"\"\"\n",
    "> üíæ **Optionally:**  \n",
    "> Save this notebook to your **personal Google Drive** to persist any changes.\n",
    ">\n",
    "> *Go to `File ‚ñ∏ Save a copy in Drive` before editing.*\n",
    "            \"\"\"\n",
    "            )\n",
    "        )\n",
    "    except Exception:\n",
    "        print(\n",
    "            \"\\nüíæ Optionally: Save the notebook to your personal Google Drive to persist changes.\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mount google drive to store data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Data Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify the following paths if necessary.**\n",
    "\n",
    "That is where your data will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if IN_COLAB:\n",
    "    DATA_PATH = Path(\"/content/drive/MyDrive/cas-dl-module-compvis-part1\")\n",
    "else:\n",
    "    DATA_PATH = Path(\"../../data\")\n",
    "assert DATA_PATH.exists(), f\"PATH: {DATA_PATH} does not exist.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Lectures Package\n",
    "\n",
    "Install `dl_cv_lectures` package with all necessary dependencies.\n",
    "\n",
    "This package provides the environment of the exercises-repository, as well as helper- and utils modules: [Link](https://github.com/marco-willi/cas-dl-compvis-exercises-hs2025)\n",
    "\n",
    "The following code installs the package from a local repository (if available), otherwise it installs it from the exercise repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "\n",
    "def ensure_dl_cv_lectures():\n",
    "    \"\"\"Ensure dl_cv_lectures is installed (local or from GitHub).\"\"\"\n",
    "    try:\n",
    "        import dl_cv_lectures\n",
    "\n",
    "        console.print(\n",
    "            \"[bold green]‚úÖ dl_cv_lectures installed ‚Äî all good![/bold green]\"\n",
    "        )\n",
    "        return\n",
    "    except ImportError:\n",
    "        console.print(\"[bold yellow]‚ö†Ô∏è dl_cv_lectures not found.[/bold yellow]\")\n",
    "    repo_path = Path(\"/workspace/pyproject.toml\")\n",
    "    if repo_path.exists():\n",
    "        console.print(\"[cyan]üì¶ Installing from local repository...[/cyan]\")\n",
    "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \"/workspace\"]\n",
    "    else:\n",
    "        console.print(\"[cyan]üåê Installing from GitHub repository...[/cyan]\")\n",
    "        cmd = [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"git+https://github.com/marco-willi/cas-dl-compvis-exercises-hs2025\",\n",
    "        ]\n",
    "    try:\n",
    "        subprocess.run(cmd, check=True)\n",
    "        console.print(\"[bold green]‚úÖ Installation successful![/bold green]\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        console.print(f\"[bold red]‚ùå Installation failed ({e}).[/bold red]\")\n",
    "\n",
    "\n",
    "ensure_dl_cv_lectures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries\n",
    "\n",
    "Load all libraries and packages used in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from collections.abc import Callable\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "import torchinfo\n",
    "import torchshow as ts\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.v2 import functional as TF\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a default device for your computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Datasets: Define and Visualize\n",
    "\n",
    "A dataset is a collection of observations (including labels) to train, evaluate or test a model.\n",
    "\n",
    "Obtaining, organizing and defining datasets is an important step in data modeling. In the following, you will use PyTorch classes to create such datasets.\n",
    "\n",
    "You can find more information in this tutorial:  [https://pytorch.org/tutorials/beginner/basics/data_tutorial.html](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
    "\n",
    "In particular, you should be familiar with, understand, and be able to use the classes [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) and [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset from torchvision\n",
    "\n",
    "We can easily pull / create a dataset from the [torchvision](https://pytorch.org/vision/stable/index.html) library. \n",
    "\n",
    "This package contains pre-packaged datasets which can be used for academic or testing purposes.\n",
    "\n",
    "We download and create the `torchvision.datasets.MNIST` dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mnist_train = torchvision.datasets.MNIST(\n",
    "    root=DATA_PATH,\n",
    "    train=True,\n",
    "    download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, the ready-made datasets come already with train and test splits.\n",
    "\n",
    "We also obtain the `test` split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mnist_test = torchvision.datasets.MNIST(root=DATA_PATH, train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at individual observations\n",
    "\n",
    "[torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) is either an iterable object or a map-style object (more common). The differences are described here [Link](https://pytorch.org/docs/stable/data.html#dataset-types).\n",
    "\n",
    "Our MNIST dataset is a map-style dataset (`_ _ getitem _ _` method implemented). This means you can retrieve any item using its key or index. We can also inspect its size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the first element of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mnist_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What makes up an observation in this case?\n",
    "<details><summary>Show Answer (example: click to reveal)</summary>\n",
    "\n",
    "An observation consists of a tuple: (image, label), where the image is a PIL.Image and the label is an integer class.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How many elements are in the dataset? Check with `len()`\n",
    "<details><summary>Show Answer</summary>\n",
    "\n",
    "The MNIST training dataset has 60,000 elements, and the test set has 10,000 elements.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What makes up an observation in this case?\n",
    "<details><summary>Show Answer (example: click to reveal)</summary>\n",
    "\n",
    "An observation consists of a tuple: (image, label), where the image is a PIL.Image and the label is an integer class.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_image, label = ds_mnist_train[0]\n",
    "pil_image\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use [matplotlib](https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.html) to visualize images. The most commonly used plotting library in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "_ = ax.imshow(pil_image, cmap=\"Greys_r\")\n",
    "_ = ax.axis(\"off\")\n",
    "_ = ax.set_title(f\"LABEL: {label}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use [torchshow](https://github.com/xwying/torchshow). Be aware to convert the image to a `torch.Tensor` first. And scale the values between 0 and 1.\n",
    "`torchshow` provides a convenient way to visualize tensors. It also allows for visualizing batches of images.\n",
    "\n",
    "We use [torchvision.transforms.v2.functional](https://pytorch.org/vision/main/transforms.html#v2-api-reference-recommended) to convert a PIL image to a torch.Tensor.\n",
    "\n",
    "This module contains many useful image processing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = TF.to_image(pil_image).to(torch.float32) / 255.0\n",
    "ts.show(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to look at multiple images, to get a feeling for the dataset.\n",
    "\n",
    "Let's collect a couple of images. For convenience save them as tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_from_ds(\n",
    "    ds: torch.utils.data.Dataset, num_images_to_fetch: int = 16\n",
    ") -> list[torch.Tensor]:\n",
    "    \"\"\"Fetch first n images from a torch.utils.data.Dataset with (image, label) signature.\"\"\"\n",
    "    # for each image: convert it to (N x C x H x W) format and scale to 0-1\n",
    "    images = [\n",
    "        TF.to_image(ds[i][0]).to(torch.float32).unsqueeze(0) / 255.0\n",
    "        for i in range(0, num_images_to_fetch)\n",
    "    ]\n",
    "    return images\n",
    "\n",
    "\n",
    "images = get_images_from_ds(ds_mnist_train, num_images_to_fetch=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convenient way to display a grid of images is either `torchshow` or `torchvision.utils.make_grid`.\n",
    "\n",
    "We create a batch of images by concatenating them into a tensor of (N x C x H x W).\n",
    "\n",
    "**Note**: The most common way to organize images in PyTorch is the **NCHW** format, indicating the first dimension is the batch dimension, followed by the color channels, and height and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch = torch.concat(images)\n",
    "x_batch.shape\n",
    "ts.show(x_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we can create one big tensor with the images arranged in a grid and then visualize that using [torchvision.utils.make_grid](https://pytorch.org/vision/main/generated/torchvision.utils.make_grid.html).\n",
    "\n",
    "This gives us more control on how we want to arrange the images (`nrow` is the number of images per row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_grid = make_grid(x_batch, nrow=2)\n",
    "ts.show(image_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra Task**: Visualize the images with their label on top of it to inspect label noise / correctness.\n",
    "\n",
    "You can use the following function to do the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_square_collage_with_captions(\n",
    "    images: list[torch.Tensor], captions: list[str], caption_width: int = 30\n",
    "):\n",
    "    \"\"\"Plot Square collage of images with captions on to of each image.\"\"\"\n",
    "    import math\n",
    "    from textwrap import wrap\n",
    "\n",
    "    num_images = len(images)\n",
    "    side_length = math.ceil(math.sqrt(num_images))\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for i in range(num_images):\n",
    "        ax = plt.subplot(side_length, side_length, i + 1)\n",
    "        caption = captions[i]\n",
    "        caption = \"\\n\".join(wrap(caption, caption_width))\n",
    "        plt.title(caption)\n",
    "        pil_image = TF.to_pil_image(images[i])\n",
    "        plt.imshow(pil_image)\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra Task**: Produce and inspect the distribution over the labels. This is important information and informs the whole modelling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Data Transforms\n",
    "\n",
    "We often need to transform / pre-process our data. \n",
    "\n",
    "For example, we might want to scale the data such that the mean is 0 and the standard deviation is 1, or we want to bring the data into the $[0,1]$ interval (by dividing by 255.).\n",
    "\n",
    "Often we also want to apply `data augmentation` techniques to add more variation to our data.\n",
    "\n",
    "**We will look into this topic more at a later stage.** Below a small examle.\n",
    "\n",
    "Torchvision provides us with many options to implement transformations: [torchvision.transforms](https://pytorch.org/vision/main/transforms.html#)\n",
    "\n",
    "Here is a small example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/pytorch/vision/blob/main/gallery/assets/dog2.jpg?raw=true\"\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "image = Image.open(io.BytesIO(r.content))\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a transformation pipeline with random components and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "composed_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomRotation(45),\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.Resize((512, 512), antialias=True),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "num_examples = 16\n",
    "images = []\n",
    "for _ in range(0, num_examples):\n",
    "    images.append(composed_transforms(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we visualize the effect of the transforms on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c783cb332598b935",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "outputs": [],
   "source": [
    "images_batch = torch.stack(images, axis=0).to(torch.float32)\n",
    "images_batch.shape\n",
    "ts.show(images_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Play around with different versions of the `composed_transforms` object. You find inspiration here: [torchvision.transforms](https://pytorch.org/vision/0.9/transforms.html#).\n",
    "\n",
    "A common operation is, e.g. a center crop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders\n",
    "\n",
    "To train models we need to define a pipeline that reads, transforms and batches images.\n",
    "\n",
    "To achieve this we use the [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class.\n",
    "\n",
    "We can wrap a `torch.utils.data.Dataset` within a `torch.utils.data.DataLoader`.\n",
    "\n",
    "Note the options: `batch_size`, `shuffle` and `num_workers` which are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_mnist_train = torch.utils.data.DataLoader(\n",
    "    ds_mnist_train, batch_size=12, shuffle=True, num_workers=4\n",
    ")\n",
    "\n",
    "dl_mnist_test = torch.utils.data.DataLoader(\n",
    "    ds_mnist_test, batch_size=12, shuffle=False, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the first batch\n",
    "try:\n",
    "    for images, labels in dl_mnist_train:\n",
    "        break\n",
    "except TypeError as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "ts.show(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**  \n",
    "This does not work. Why?\n",
    "\n",
    "<details>\n",
    "<summary><b>üí° Click to show answer</b></summary>\n",
    "\n",
    "**Answer:**  \n",
    "The `Dataset` object returns `PIL.Image.Image` objects which cannot be batched.  \n",
    "We need to transform the images to tensors.\n",
    "\n",
    "We can use the `torchvision.transforms` module for that.  \n",
    "`torchvision.datasets` offers a convenient way to apply transformations to images in a `Dataset` object.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mnist_train = torchvision.datasets.MNIST(\n",
    "    root=DATA_PATH, train=True, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "dl_mnist_train = torch.utils.data.DataLoader(\n",
    "    ds_mnist_train, batch_size=12, shuffle=True, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check if it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the first batch\n",
    "try:\n",
    "    for images, labels in dl_mnist_train:\n",
    "        break\n",
    "except TypeError as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "ts.show(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We are have now a data pipeline which can be used in model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Network Definition\n",
    "\n",
    "Now, that we have prepared our data: We want to model it. \n",
    "\n",
    "First, we need to define our model: Let's impement a shallow Multilayer-Perceptron.\n",
    "\n",
    "We use the [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class to define networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"A Multi-Layer Perceptron (MLP) model for classification.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hidden: int,\n",
    "        num_classes: int,\n",
    "        input_size: tuple[int, int, int] = (1, 28, 28),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_hidden (int): Number of neurons in the hidden layer.\n",
    "            num_classes (int): Number of output classes for classification.\n",
    "            input_size tuple[int, int, int]: The dimensions of the input image.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Flatten the input image into a 1D tensor\n",
    "        # for example, from (1, 28, 28) to (784,)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Hidden layer: fully connected layer from input_size to num_hidden neurons.\n",
    "        # No bias is used here (bias=False).\n",
    "        self.hidden = nn.Linear(\n",
    "            in_features=input_size[0] * input_size[1] * input_size[2],\n",
    "            out_features=num_hidden,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        # Output layer: fully connected layer from num_hidden neurons to num_classes outputs.\n",
    "        self.output = nn.Linear(in_features=num_hidden, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the MLP model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits (before softmax).\n",
    "        \"\"\"\n",
    "        # Flatten the input tensor into (batch_size, input_size[0] * input_size[1])\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Apply the hidden layer (linear transformation)\n",
    "        x = self.hidden(x)\n",
    "\n",
    "        # Apply ReLU activation function to introduce non-linearity\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Apply the output layer (linear transformation) to get the logits\n",
    "        x = self.output(x)\n",
    "\n",
    "        # Return the output logits (not yet passed through softmax)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets initialize the model and inspect it using `torchinfo`.\n",
    "\n",
    "Note: We need to define the input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP(num_hidden=64, num_classes=10)\n",
    "net = net.to(device)\n",
    "print(net)\n",
    "print(torchinfo.summary(net, input_size=(1, 1, 28, 28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How many parameters does the model have?\n",
    "<details><summary>Show Answer</summary>\n",
    "\n",
    "For the default MLP with input size (1, 28, 28), 64 hidden units, and 10 output classes:\n",
    "- Input to hidden: 1√ó28√ó28√ó64 = 50,176 (no bias)\n",
    "- Hidden to output: 64√ó10 = 640\n",
    "- Output layer bias: 10\n",
    "- **Total: 50,826 parameters**\n",
    "\n",
    "</details>\n",
    "\n",
    "**Question**: How many would it have if there were only 5 classes? (`num_classes` from 10 to 5).\n",
    "<details><summary>Show Answer</summary>\n",
    "\n",
    "- Input to hidden: 1√ó28√ó28√ó64 = 50,176 (no bias)\n",
    "- Hidden to output: 64√ó5 = 320\n",
    "- Output layer bias: 5\n",
    "- **Total: 50,501 parameters**\n",
    "\n",
    "The difference is 325 parameters (from the output layer).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the model works. E.g. if we can perform a forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_input = torch.randn(size=(1, 1, 28, 28)).to(device)\n",
    "\n",
    "output = net(random_input)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What would be the output.shape if we change to `torch.randn(size=(5, 1, 28, 28))`?\n",
    "<details><summary>Show Answer</summary>\n",
    "\n",
    "The output shape would be **(5, 10)** - 5 images in the batch, 10 classes (logits for each class).\n",
    "\n",
    "</details>\n",
    "\n",
    "**Question**: What would be the output.shape if we change to `torch.randn(size=(1, 3, 28, 28))`?\n",
    "<details><summary>Show Answer</summary>\n",
    "\n",
    "This would cause an error! The model expects input with shape (batch_size, 1, 28, 28), but we're providing (1, 3, 28, 28) with 3 channels instead of 1. The flatten operation would produce 3√ó28√ó28 = 2,352 features, but the hidden layer expects 1√ó28√ó28 = 784 features.\n",
    "\n",
    "**Error**: RuntimeError: size mismatch\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Model Training\n",
    "\n",
    "We need different components to train a model:\n",
    "- Loss function and optimizer\n",
    "- Trainer: Iteration over training data and parameter updates\n",
    "- Monitoring of the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function & Optimizer\n",
    "\n",
    "We need to define a loss function and an optimizer for the model which defines how the parameters are updated.\n",
    "\n",
    "We use the Cross-Entropy Loss which is common for classification problems (more on that later)\n",
    "\n",
    "To optimize the neural network weights we use the popular Adam optimizer with regularization (weight decay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(net.parameters(), weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "There are different ways to implement the training loop. \n",
    "\n",
    "It is typically a lot of boiler-plate code, thats why there are higher-level APIs such as:\n",
    "\n",
    "- [lightning](https://lightning.ai/docs/pytorch/stable/)\n",
    "- [Keras](https://keras.io/keras_3/)\n",
    "\n",
    "And also use metrics tracker from libraries such as [torchmetrics](https://github.com/Lightning-AI/torchmetrics).\n",
    "\n",
    "It is important to monitor training-progress. Thats why we calculate and print metrics.\n",
    "\n",
    "We are, however, implementing the boiler plate below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    net: torch.nn.Module,\n",
    "    optimizer: torch.optim,\n",
    "    loss_fn: Callable,\n",
    "    device: str = \"cpu\",\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    net = net.to(device)\n",
    "\n",
    "    with tqdm(data_loader, unit=\"batch\", disable=not verbose) as tepoch:\n",
    "        total_samples_seen = 0\n",
    "        total_correct = 0\n",
    "\n",
    "        for step, (X, y) in enumerate(tepoch):\n",
    "            # Update Step\n",
    "            logits = net(X.to(device))\n",
    "            loss = loss_fn(logits, y.to(device))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate Accuracy\n",
    "            class_probabilities = torch.softmax(logits, axis=-1).detach().cpu()\n",
    "            y_hat = (\n",
    "                class_probabilities.argmax(dim=1, keepdim=True).squeeze().detach().cpu()\n",
    "            )\n",
    "\n",
    "            num_correct = (y_hat == y).sum().item()\n",
    "            num_samples = X.shape[0]\n",
    "            batch_accuracy = num_correct / num_samples\n",
    "\n",
    "            # Epoch Statistics\n",
    "            total_samples_seen += num_samples\n",
    "            total_correct += num_correct\n",
    "            epoch_accuracy = total_correct / total_samples_seen\n",
    "\n",
    "            if verbose:\n",
    "                tepoch.set_postfix(\n",
    "                    loss=loss.item(),\n",
    "                    accuracy_batch=batch_accuracy,\n",
    "                    accuracy_epoch=epoch_accuracy,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs = 5\n",
    "for epoch in range(0, total_epochs):\n",
    "    print(f\"Starting Epoch: {epoch + 1} / {total_epochs}\")\n",
    "    train_one_epoch(dl_mnist_train, net, optimizer, loss_fn, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the progress the model is making with respect to accuracy and the loss function value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Evaluation\n",
    "\n",
    "A very important part is model evaluation. We can not rely on the training scores for model evaluation since it might be too optimistic due to overfitting.\n",
    "\n",
    "We evaluate our (best) model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    net: torch.nn.Module,\n",
    "    loss_fn: Callable,\n",
    "    device: str = \"cpu\",\n",
    ") -> tuple[float, torch.Tensor, torch.Tensor]:\n",
    "    net = net.to(device)\n",
    "    net.eval()\n",
    "    with tqdm(data_loader, unit=\"batch\") as tepoch:\n",
    "        total_samples_seen = 0\n",
    "        total_correct = 0\n",
    "\n",
    "        y_list = list()\n",
    "        y_hat_list = list()\n",
    "\n",
    "        for step, (X, y) in enumerate(tepoch):\n",
    "            # Forward Pass\n",
    "            with torch.no_grad():\n",
    "                logits = net(X.to(device))\n",
    "            loss = loss_fn(logits, y.to(device))\n",
    "\n",
    "            # Predictions\n",
    "            class_probabilities = torch.softmax(logits, axis=-1).detach().cpu()\n",
    "            y_hat = (\n",
    "                class_probabilities.argmax(dim=1, keepdim=True).squeeze().detach().cpu()\n",
    "            )\n",
    "\n",
    "            # Metrics\n",
    "            num_correct = (y_hat == y).sum().item()\n",
    "            num_samples = X.shape[0]\n",
    "            total_samples_seen += num_samples\n",
    "            total_correct += num_correct\n",
    "            epoch_accuracy = total_correct / total_samples_seen\n",
    "\n",
    "            tepoch.set_postfix(\n",
    "                loss=loss.item(),\n",
    "                accuracy_epoch=epoch_accuracy,\n",
    "            )\n",
    "\n",
    "            # save preds and targets\n",
    "            y_list.append(y.cpu())\n",
    "            y_hat_list.append(y_hat.cpu())\n",
    "\n",
    "    return epoch_accuracy, torch.concat(y_list), torch.concat(y_hat_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When testing and evaluating we need to apply exactly the same (static) transformations (not the data augmentation!) as when training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mnist_test = torchvision.datasets.MNIST(\n",
    "    root=DATA_PATH, train=False, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "dl_mnist_test = torch.utils.data.DataLoader(\n",
    "    ds_mnist_test, batch_size=128, shuffle=False, num_workers=4\n",
    ")\n",
    "\n",
    "test_accuracy, y, y_hat = eval_loop(dl_mnist_test, net, loss_fn, device=device)\n",
    "\n",
    "print(f\"Test Accuracy:  {test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also inspect the confusion matrix using the sklearn confusion matrix [Link](https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.confusion_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y, y_hat)\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_display.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
