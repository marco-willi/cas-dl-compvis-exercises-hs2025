{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Image Classification - A Practical Approach\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "- Learn how to model an image classification task\n",
    "- Learn how to systematically implement data prep, model architecture, training-loop, and evaluation\n",
    "- Learn how to check and verify the implementation\n",
    "- Learn how to incorporate boilerplate code from [torchvision](https://pytorch.org/vision/0.9/index.html) and  [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9856bbf98ea570f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Google Colab Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Detect Colab\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "print(f\"In Colab: {IN_COLAB}\")\n",
    "\n",
    "# Show prominent message if in Colab\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        from IPython.display import Markdown, display\n",
    "\n",
    "        display(\n",
    "            Markdown(\n",
    "                \"\"\"\n",
    "> ðŸ’¾ **Optionally:**  \n",
    "> Save this notebook to your **personal Google Drive** to persist any changes.\n",
    ">\n",
    "> *Go to `File â–¸ Save a copy in Drive` before editing.*\n",
    "            \"\"\"\n",
    "            )\n",
    "        )\n",
    "    except Exception:\n",
    "        print(\n",
    "            \"\\nðŸ’¾ Optionally: Save the notebook to your personal Google Drive to persist changes.\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mount google drive to store data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Data Path\n",
    "\n",
    "**Modify the following paths if necessary.**\n",
    "\n",
    "That is where your data will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if IN_COLAB:\n",
    "    DATA_PATH = Path(\"/content/drive/MyDrive/cas-dl-module-compvis-part1\")\n",
    "else:\n",
    "    DATA_PATH = Path(\"../../data\")\n",
    "assert DATA_PATH.exists(), f\"PATH: {DATA_PATH} does not exist.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Lectures Package\n",
    "\n",
    "Install `dl_cv_lectures` package with all necessary dependencies.\n",
    "\n",
    "This package provides the environment of the exercises-repository, as well as helper- and utils modules: [Link](https://github.com/marco-willi/cas-dl-compvis-exercises-hs2025)\n",
    "\n",
    "The following code installs the package from a local repository (if available), otherwise it installs it from the exercise repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "\n",
    "def ensure_dl_cv_lectures():\n",
    "    \"\"\"Ensure dl_cv_lectures is installed (local or from GitHub).\"\"\"\n",
    "    try:\n",
    "        import dl_cv_lectures\n",
    "\n",
    "        console.print(\n",
    "            \"[bold green]âœ… dl_cv_lectures installed â€” all good![/bold green]\"\n",
    "        )\n",
    "        return\n",
    "    except ImportError:\n",
    "        console.print(\"[bold yellow]âš ï¸ dl_cv_lectures not found.[/bold yellow]\")\n",
    "    repo_path = Path(\"/workspace/pyproject.toml\")\n",
    "    if repo_path.exists():\n",
    "        console.print(\"[cyan]ðŸ“¦ Installing from local repository...[/cyan]\")\n",
    "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \"/workspace\"]\n",
    "    else:\n",
    "        console.print(\"[cyan]ðŸŒ Installing from GitHub repository...[/cyan]\")\n",
    "        cmd = [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"git+https://github.com/marco-willi/cas-dl-compvis-exercises-hs2025\",\n",
    "        ]\n",
    "    try:\n",
    "        subprocess.run(cmd, check=True)\n",
    "        console.print(\"[bold green]âœ… Installation successful![/bold green]\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        console.print(f\"[bold red]âŒ Installation failed ({e}).[/bold red]\")\n",
    "\n",
    "\n",
    "ensure_dl_cv_lectures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries\n",
    "\n",
    "Load all libraries and packages used in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchinfo\n",
    "import torchshow as ts\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import v2 as transforms\n",
    "\n",
    "from dl_cv_lectures.utils import find_all_imges_and_their_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a default device for your computations.\n",
    "\n",
    "**GPU is strongly recommended!** (otherwise the images have to be restricted in size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change the following parameter to 0**: `DEV_RUNS=0`\n",
    "\n",
    "Otherwise model training code will only run a small number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the following parameter to 0, otherwise model training is only conducted for a small number of steps to test the code\n",
    "DEV_RUNS = 0\n",
    "\n",
    "DEBUG = DEV_RUNS > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "projects",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1) Project Selection\n",
    "\n",
    "Choose one of the following projects to work on. \n",
    "\n",
    "All datasets are structured similarily in class-specific image folders.\n",
    "\n",
    "This notebook was thoroughly tested with the cats vs dogs dataset and uses this one as an example. \n",
    "\n",
    "**Choose Cats vs Dogs if you want to mainly click through the notebook.**\n",
    "\n",
    "All other datasets require some (minor!) adaptations. Feel free to choose whichever interests you most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cats vs Dogs\n",
    "\n",
    "**Goal**: Develop a model to classify images of cats and dogs. The dataset is designed to facilitate the identification of these animals from images.\n",
    "\n",
    "**Approach**: Create a Convolutional Neural Network (CNN) to classify the images into two categories: cats and dogs. Experiment with various CNN architectures and techniques to determine the most effective method. Use data augmentation techniques to handle variations in pose, lighting, and background. Ensure the model generalizes well by using cross-validation and monitoring for overfitting.\n",
    "\n",
    "**Dataset**: The dataset contains 25,000 images, with approximately 12,500 images per class (cats and dogs). Each image varies in size and resolution. The data is provided by Microsoft as part of their Kaggle competition.\n",
    "\n",
    "[Source](https://www.microsoft.com/en-us/download/details.aspx?id=54765)\n",
    "\n",
    "![Dog](dog.jpg)\n",
    "![Cat](cat.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concrete Crack Detection\n",
    "\n",
    "**Goal**: Develop a model to classify concrete images as having cracks or not. The dataset is designed to facilitate the identification of structural issues in concrete buildings.\n",
    "\n",
    "**Approach**: Create a Convolutional Neural Network (CNN) to classify the images into negative (no crack) and positive (crack) categories. Experiment with various CNN architectures and techniques to determine the most effective method. Use image processing techniques to handle variations in surface finish and illumination. Ensure the model generalizes well by using cross-validation and monitoring for overfitting.\n",
    "\n",
    "**Dataset**: The dataset contains 40,000 images, with 20,000 images per class (negative and positive). Each image is 227 x 227 pixels with RGB channels. The data is collected from 458 high-resolution images (4032 x 3024 pixels) from various METU Campus Buildings. No data augmentation such as random rotation or flipping is applied.\n",
    "\n",
    "[Source](https://data.mendeley.com/datasets/5y9wdsg2zt/2)\n",
    "\n",
    "![Crack](crack_example.jpg)\n",
    "![No Crack](crack_negative.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scene Classification\n",
    "\n",
    "**Goal**: Develop a model to classify natural scene images into one of six categories. The dataset aims to facilitate the recognition of various natural scenes from around the world.\n",
    "\n",
    "**Approach**: Design a Convolutional Neural Network (CNN) to classify images into six categories: buildings, forest, glacier, mountain, sea, and street. Test different CNN architectures to find the best performing model. Apply data augmentation techniques to improve generalization. Separate the data into training, testing, and prediction sets to evaluate model performance effectively.\n",
    "\n",
    "**Dataset**: The dataset contains around 25,000 images of size 150 x 150 pixels, distributed across six categories. The data is separated into training (14,000 images), testing (3,000 images), and prediction (7,000 images) sets.\n",
    "\n",
    "[Source](https://www.kaggle.com/datasets/puneet6060/intel-image-classification?resource=download)\n",
    "\n",
    "\n",
    "![Builings](natural_scenes_buildings.jpg)\n",
    "![Forest](natural_scenes_forest.jpg)\n",
    "![Glacier](natural_scenes_glacier.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satellite Land Cover Classification\n",
    "\n",
    "**Goal**: Develop a model to classify satellite images into different land cover types. The dataset contains images of 10 different classes and aims to support land use and land cover classification tasks.\n",
    "\n",
    "**Approach**: Develop Convolutional Neural Networks (CNNs) to model the satellite image data. Experiment with different CNN architectures to identify the best performing model. Compare pre-trained models with those trained from scratch. Use data augmentation techniques to enhance model generalization. Given the relatively small dataset, pay attention to overfitting and compare models robustly.\n",
    "\n",
    "**Dataset**: The dataset consists of 27,000 RGB images categorized into 10 classes. The dataset is available in two formats: one in RGB and another with 13 spectral bands. Use the RGB dataset for this project.\n",
    "\n",
    "[Source](https://github.com/phelber/eurosat)\n",
    "\n",
    "![Crop](sat_crop.jpg)\n",
    "![Forest](sat_forest.jpg)\n",
    "![Highway](sat_highway.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose your own dataset!\n",
    "\n",
    "Feel free to choose your own dataset.\n",
    "\n",
    "If you do so. It is best to organzie the data in image folders like this:\n",
    "\n",
    "```\n",
    "dataset/\n",
    "â”œâ”€â”€ class1/\n",
    "â”‚   â”œâ”€â”€ img1.jpg\n",
    "â”‚   â”œâ”€â”€ img2.jpg\n",
    "â”‚   â””â”€â”€ ...\n",
    "â”œâ”€â”€ class2/\n",
    "â”‚   â”œâ”€â”€ img1.jpg\n",
    "â”‚   â”œâ”€â”€ img2.jpg\n",
    "â”‚   â””â”€â”€ ...\n",
    "â””â”€â”€ ...\n",
    "```\n",
    "If you don't have train/val folders this is fine. The separation can be made later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Overall Approach\n",
    "\n",
    "Inspired by [A Recipe for Training Neural Networks by Andrej Karpathy](https://karpathy.github.io/2019/04/25/recipe/)\n",
    "\n",
    "For your chosen dataset. Do the following:\n",
    "\n",
    "\n",
    "### 1) Data Preparation & Data Inspection\n",
    "\n",
    "- Download the data\n",
    "- Inspect the data formats\n",
    "- Build a `torch.utils.data.Dataset`\n",
    "    - define training, validation and test sets\n",
    "- Implement a `torch.utils.data.DataLoader'\n",
    "- Inspect the data:\n",
    "    - Look at samples\n",
    "    - Inspect the label distribution\n",
    "\n",
    "### 2) Baselines\n",
    "\n",
    "- Implement a small CNN\n",
    "- Learn input-independent baseline (provide only labels but random noise as input)\n",
    "- Overfitt CNN on one batch\n",
    "- Inspect pre-processing\n",
    "  \n",
    "### 3) (Over)fit\n",
    "- Build a large(er) architecture (pre-trained or self-implemented)\n",
    "- Train a high-performing model with respect to training set\n",
    "  \n",
    "### 4) Regularize\n",
    "- Is it beneficial to collect more data?\n",
    "- Data Augmentation\n",
    "- Early Stopping on Validation Set\n",
    "- Weight Decay\n",
    "\n",
    "### 5) Hyper-Parameter Tuning\n",
    "- Define HPs and parameterise architecture\n",
    "- do grid- or random search over HP grids\n",
    "\n",
    "### 6) Squeeze out the juice\n",
    "-  Ensembling\n",
    "-  Longer training\n",
    "-  Special techniques: AdamW optimizer, fancy data augmentation, label smoothing, stochastic depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1) - Data Preparation & Data Inspection\n",
    "\n",
    "**_This step is critical. I like to spend copious amount of time (measured in units of hours) scanning through thousands of examples, understanding their distribution and looking for patterns. (A Karpathy)_**\n",
    "\n",
    "- Download the data\n",
    "- Inspect the data formats and file organization\n",
    "- Remove corrupt data\n",
    "- Build a [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)\n",
    "    - define training, validation and test sets\n",
    "- Implement a [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n",
    "- Inspect the data:\n",
    "    - Look at samples\n",
    "    - Inspect the label distribution\n",
    "\n",
    "\n",
    "Important information about how to define a dataset can be found here: [https://pytorch.org/docs/stable/data.htm](https://pytorch.org/docs/stable/data.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Data\n",
    "\n",
    "Specify the dataset that you want to use and download it (unless you are using your own - in that case place it in your data directory).\n",
    "\n",
    "Download and extraction may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_cv_lectures.data import (\n",
    "    cats_vs_dogs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eurosat.download(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the \"cats vs dogs\" dataset. Feel free to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_vs_dogs.download(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Data Format & Organization\n",
    "\n",
    "We need to figure out how the data is organized. Particularly, how the data is labelled, to correctly define it with a [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset).\n",
    "\n",
    "First you should look at the data / folder structure of the downloaded data.\n",
    "\n",
    "(this example command only works on Linux)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!find {DATA_PATH}/cats_vs_dogs/PetImages/ -type d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the data is neatly organized in class-specific folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!find {DATA_PATH}/cats_vs_dogs/PetImages/Cat/ -type f | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like we have \".jpg\" files. This needs to be verified further.\n",
    "\n",
    "**Note**: You might want to check for corrupt files since they disrupt further processes. You could read each file using `PIL.Image.open` to find any errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a function to provide an inventory of all the files, along with their labels.\n",
    "\n",
    "Change the path accordingly if you used a different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_root_path = DATA_PATH.joinpath(\"cats_vs_dogs/PetImages\")\n",
    "\n",
    "observations = find_all_imges_and_their_labels(image_root_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: How many images are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Dataset class\n",
    "\n",
    "Once you have figured out how the data is organized we can build a [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset). \n",
    "\n",
    "In this case we can build a map-style dataset that allows for random-access and implements `__getitem__`. We subclass `torch.utils.data.Dataset`.\n",
    "\n",
    "**Note**: Note the option to use a `transform` function that is being applied to each image. This is going to be important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolder(Dataset):\n",
    "    \"\"\"Create Dataset from class specific folders.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_path: str | Path,\n",
    "        transform: Callable | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_path: Path to directory that contains the class-specific folders\n",
    "            transform: Optional transform to be applied on an image\n",
    "            classes: List of class names.\n",
    "        \"\"\"\n",
    "        self.root_path = root_path\n",
    "        self.observations = find_all_imges_and_their_labels(root_path)\n",
    "        self.transform = transform\n",
    "        self.classes = sorted({x[\"label\"] for x in self.observations})\n",
    "        print(\n",
    "            f\"Found the following classes: {self.classes}, in total {len(self.observations)} images\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.observations)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_path = self.observations[idx][\"image_path\"]\n",
    "        image = Image.open(image_path)\n",
    "        label = self.observations[idx][\"label\"]\n",
    "        label_num = self.classes.index(label)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return {\"image\": image, \"label\": label_num}\n",
    "\n",
    "    @classmethod\n",
    "    def from_subset(\n",
    "        cls,\n",
    "        original_dataset,\n",
    "        subset_indices: list[int],\n",
    "        transform: Callable | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a subset of the original dataset with only the specified indices.\n",
    "\n",
    "        Args:\n",
    "            original_dataset (ImageFolder): An instance of the ImageFolder dataset.\n",
    "            subset_indices (List[int]): List of indices to create a subset of observations.\n",
    "            transform: Override transform of current ds\n",
    "\n",
    "        Returns:\n",
    "            ImageFolder: A new instance of ImageFolder with the subset observations.\n",
    "        \"\"\"\n",
    "        # Create a new instance with the same properties as the original\n",
    "        subset_instance = cls(\n",
    "            root_path=original_dataset.root_path,\n",
    "            transform=original_dataset.transform if transform is None else transform,\n",
    "        )\n",
    "\n",
    "        # Filter the observations based on the subset indices\n",
    "        subset_instance.observations = [\n",
    "            original_dataset.observations[i] for i in subset_indices\n",
    "        ]\n",
    "        subset_instance.classes = original_dataset.classes  # Keep class list consistent\n",
    "\n",
    "        print(\n",
    "            f\"Created a subset with {len(subset_instance.observations)} images \"\n",
    "            f\"from the original dataset of {len(original_dataset.observations)} images\"\n",
    "        )\n",
    "\n",
    "        return subset_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What is the role of: `label_num = self.classes.index(label)`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the dataset object and inspect the first observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ImageFolder(image_root_path)\n",
    "ds.classes\n",
    "ds[0]\n",
    "ds[0][\"image\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train / Test Splits\n",
    "\n",
    "We create train (validation) and test splits early, before we look at the data in more detail. This avoids any biases and insights that we might clean from the test set. We should set this part of the data aside and look at it only for a final model evaluation.\n",
    "\n",
    "We use a function from `sklearn.model_selection`.\n",
    "\n",
    "We balance the labels by using the `stratify` option.\n",
    "\n",
    "**Note**: Some datasets may have a pre-defined split. If so, the images might be in separate test/train folders. If that is the case simply create different dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = list(range(0, len(ds.observations)))\n",
    "all_labels = [x[\"label\"] for x in ds.observations]\n",
    "\n",
    "train_ids, test_ids = train_test_split(\n",
    "    all_ids,\n",
    "    stratify=all_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=123,\n",
    ")\n",
    "\n",
    "train_ids, val_ids = train_test_split(\n",
    "    train_ids,\n",
    "    stratify=[all_labels[i] for i in train_ids],\n",
    "    test_size=0.1,\n",
    "    random_state=123,\n",
    ")\n",
    "\n",
    "\n",
    "ds_train = ImageFolder.from_subset(ds, train_ids)\n",
    "ds_val = ImageFolder.from_subset(ds, val_ids)\n",
    "ds_test = ImageFolder.from_subset(ds, test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-974feaab57d71ef1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: Why do we need a training, validation and a testset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test the `Dataset` object by getting and visualising a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = ds_train[0]\n",
    "ts.show(observation[\"image\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model training we need to batch examples. Thats why we need to define a [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n",
    "\n",
    "Let's see how our data is being batched, after all each observation is a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ds_train, batch_size=16, shuffle=True)\n",
    "\n",
    "try:\n",
    "    observations = next(iter(dl_train))\n",
    "except TypeError as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oops that did not go well!**: What did we miss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RGB(),\n",
    "        transforms.ToImage(),\n",
    "        transforms.ToDtype(torch.float32, scale=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ds_train = ImageFolder.from_subset(ds, train_ids, transform=tr_train)\n",
    "ds_val = ImageFolder.from_subset(ds, val_ids, transform=tr_train)\n",
    "ds_test = ImageFolder.from_subset(ds, test_ids, transform=tr_train)\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=16, shuffle=True)\n",
    "\n",
    "try:\n",
    "    observations = next(iter(dl_train))\n",
    "except (RuntimeError, TypeError) as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oops that did not go well!**: What did we miss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RGB(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToImage(),\n",
    "        transforms.ToDtype(torch.float32, scale=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ds_train = ImageFolder.from_subset(ds, train_ids, transform=tr_train)\n",
    "ds_val = ImageFolder.from_subset(ds, val_ids, transform=tr_train)\n",
    "ds_test = ImageFolder.from_subset(ds, test_ids, transform=tr_train)\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=16, shuffle=True)\n",
    "\n",
    "try:\n",
    "    observations = next(iter(dl_train))\n",
    "except (RuntimeError, TypeError) as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "\n",
    "observations.keys()\n",
    "\n",
    "observations[\"image\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8385aee74294e0fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: How does the DataLoader batch the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the Data\n",
    "\n",
    "Now you can use the `ImageDataset` or `DataLoader` objects to insepct the dataset. \n",
    "\n",
    "**Note**: We use only the training dataset to inspect the data.\n",
    "\n",
    "- **Initial Step**: Avoid touching neural net code initially; focus on inspecting the data thoroughly.\n",
    "- **Time Investment**: Spend hours scanning thousands of examples to understand their distribution and look for patterns.\n",
    "- **Identify Issues**: Look for duplicate examples, corrupted images/labels, data imbalances, and biases.\n",
    "- **Classify Process**: Pay attention to how you classify the data to inform the architecture exploration.\n",
    "- **Feature Analysis**: Determine if local features or global context is needed.\n",
    "- **Variation Analysis**: Assess the variation in the data, identify spurious variations for preprocessing.\n",
    "- **Spatial Consideration**: Evaluate if spatial position matters or if averaging it out is beneficial.\n",
    "- **Detail and Downsampling**: Consider the importance of detail and the feasibility of downsampling images.\n",
    "- **Label Noise**: Assess the noise level in the labels.\n",
    "- **Understand Predictions**: Use network (mis)predictions to understand inconsistencies and data issues (at a later stage!).\n",
    "- **Quantitative Analysis**: Write simple code to search, filter, and sort data by various attributes.\n",
    "- **Visualize Distributions**: Visualize distributions and outliers to uncover bugs in data quality or preprocessing.\n",
    "\n",
    "For now do at least the following:\n",
    "- what is the class distribution?\n",
    "- how difficult do you think is the problem?\n",
    "- are there any obvious issues with the data?\n",
    "- do the labels seem accurate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-556797ba20290ab9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.unique([obs[\"label\"] for obs in ds_train.observations], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Use functions to visualize and inspect the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Implement Baselines\n",
    "\n",
    "In this step we want to implement a training pipeline and evaluate simple baselines to get a feeling for the problem and to test and verify if the pipeline works.\n",
    "\n",
    "**Reproducibility**\n",
    "\n",
    "- Fix random seed: Always use a fixed random seed to ensure consistent outcomes in repeated runs.\n",
    "\n",
    "**Simplification and Initialization**\n",
    "\n",
    "- Simplify: Disable unnecessary features like data augmentation initially.\n",
    "- Verify loss at initialization: Ensure loss starts at the expected value.\n",
    "\n",
    "**Baselines and Metrics**\n",
    "\n",
    "- Human baseline: Compare model metrics to human-interpretable metrics (e.g., accuracy).\n",
    "- Input-independent baseline: Train a baseline model with zeroed inputs and compare it to a variant with normal data. There should be a clear difference!\n",
    "\n",
    "**Overfitting and Visualization**\n",
    "\n",
    "- Overfit one batch: Overfit a single batch to verify the model can reach the minimum loss.\n",
    "- Verify decreasing training loss: Ensure training loss decreases when model capacity increases.\n",
    "- Visualize before the net: Visualize data immediately before feeding it to the network to catch preprocessing issues.\n",
    "- Visualize prediction dynamics: Track model predictions on a fixed test batch during training to understand training progression.\n",
    "\n",
    "**Evaluation**\n",
    "\n",
    "- Add significant digits to your eval: Evaluate on the entire test set for accuracy.\n",
    "- Visualize: Visualize model inputs and outputs to ensure correctness.\n",
    "\n",
    "**Additional Tips**\n",
    "\n",
    "- Verify simplifications: Simplify initial setup by turning off data augmentation and complex features to reduce bugs.\n",
    "\n",
    "We will address some of the steps above. Feel free to do more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducibility\n",
    "\n",
    "The `lightning`package provides a function to set random seeds of different modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.seed_everything(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple DataLoader\n",
    "\n",
    "Implement a simple dataloader without fancy transformations. To specify transformations, use [torchvision.transforms](https://pytorch.org/vision/0.9/transforms.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple transformation\n",
    "tr_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RGB(),\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToImage(),\n",
    "        transforms.ToDtype(torch.float32, scale=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ds_train = ImageFolder.from_subset(ds, train_ids, transform=tr_train)\n",
    "dl_train = DataLoader(ds_train, batch_size=16, shuffle=True, num_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = next(iter(dl_train))\n",
    "ts.show(obs[\"image\"])\n",
    "obs[\"image\"].max()\n",
    "obs[\"image\"].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Model\n",
    "\n",
    "Start with a simple model that is (most likely) correct and should be able to learn something (quickly).\n",
    "\n",
    "For example you could implement the following architecture.\n",
    "\n",
    "- Input Shape: (3, **height**, **width**)\n",
    "- Convolution: 16 Filters, Kernel-Size 5x5\n",
    "- Pooling: Stride 2, Kernel-Size 2\n",
    "- Convolution: 32 Filter, Kernel-Size 5x5\n",
    "- Global Average Pooling\n",
    "- FC: 2 neurons (**number of classes**)\n",
    "\n",
    "Use `ReLU` activation after each convolution.\n",
    "\n",
    "Define a class which inherits from `torch.nn.Module`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-282219b1ddf201ab",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, (5, 5))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(32, 2)  # adapt to the number of classes here\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = SmallCNN()\n",
    "\n",
    "print(net)\n",
    "print(torchinfo.summary(net, input_size=(1, 3, 32, 32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-62ef301fb1d10cf0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: Briefly explain what happens with a data point during the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a training Loop\n",
    "\n",
    "We use Lightning which greatly simplifys implementing boilerplate code such as  training loops.\n",
    "\n",
    "Tutorial here: https://lightning.ai/pages/community/tutorial/step-by-step-walk-through-of-pytorch-lightning/\n",
    "\n",
    "We also include additional metrics from [torchmetrics](https://lightning.ai/docs/torchmetrics/stable/) to easily log and calculate accuracy.  Adapt `task=\"binary\"` if necessary!\n",
    "\n",
    "**Note**: Calculating the metrics incorrectly is a common source of errors. Make sure to correctly aggregate metrics across an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "\n",
    "class Classifier(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.train_accuracy = torchmetrics.Accuracy(\n",
    "            task=\"binary\"\n",
    "        )  # Adjust task if you have more than two classes\n",
    "        self.train_loss = torchmetrics.MeanMetric()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch[\"image\"], batch[\"label\"]\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # Update accuracy metric\n",
    "        self.train_accuracy(preds, y)\n",
    "        self.train_loss(loss)\n",
    "\n",
    "        self.log(\"train/acc_step\", self.train_accuracy, prog_bar=True)\n",
    "        self.log(\"train/loss_step\", self.train_loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        # log epoch metric\n",
    "        self.log(\"train/acc_epoch\", self.train_accuracy)\n",
    "        self.log(\"train/loss_epoch\", self.train_loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the following parameters accoring to your hardware if you need. As you can see, this simplifies hardware switches greatly!\n",
    "\n",
    "We want to perform a functional check only. Train the model only for 10 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.seed_everything(123)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    devices=\"auto\",\n",
    "    accelerator=\"auto\",\n",
    "    precision=\"32\",\n",
    "    fast_dev_run=False if DEV_RUNS == 0 else DEV_RUNS,\n",
    "    max_steps=10,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\"),\n",
    ")\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=128, shuffle=True, num_workers=5)\n",
    "\n",
    "net = SmallCNN()\n",
    "model = Classifier(net)\n",
    "trainer.fit(model, train_dataloaders=dl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.logged_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What is the loss at initialization / after 10 steps? Does the value make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model for longer to get a sense of the performance (increase the number of steps `max_steps` the model is training for)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.seed_everything(123)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    devices=\"auto\",\n",
    "    accelerator=\"auto\",\n",
    "    precision=\"32\",\n",
    "    max_steps=500,\n",
    "    fast_dev_run=False if DEV_RUNS == 0 else DEV_RUNS,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\"),\n",
    ")\n",
    "\n",
    "net = SmallCNN()\n",
    "model = Classifier(net)\n",
    "trainer.fit(model, train_dataloaders=dl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.logged_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What is your conclusion? Does learning take place?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn Input independent Model\n",
    "\n",
    "\n",
    "Modify the `Dataset` class such that random images, e.g. white noise, is returned. The label remains unchanged. Then train a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a1689e4300aaf490",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: What kind of loss do you expect for random image data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_cv_lectures.data.image_folder import ImageFolderRandom\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "ds_train_random = ImageFolderRandom.from_subset(ds, train_ids, transform=tr_train)\n",
    "\n",
    "# ds_train_random = ImageFolderRandom(ds_train)\n",
    "\n",
    "dl_train_random = DataLoader(\n",
    "    ds_train_random, batch_size=64, shuffle=True, num_workers=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify your work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_random = next(iter(dl_train_random))\n",
    "ts.show(obs_random[\"image\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.seed_everything(123)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    devices=\"auto\",\n",
    "    accelerator=\"auto\",\n",
    "    precision=\"32\",\n",
    "    fast_dev_run=False if DEV_RUNS == 0 else DEV_RUNS,\n",
    "    max_steps=100,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\"),\n",
    ")\n",
    "\n",
    "net = SmallCNN()\n",
    "model = Classifier(net)\n",
    "trainer.fit(model, train_dataloaders=dl_train_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Metrics:  {trainer.logged_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Were your expectations met? If not, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit on one Batch of Data\n",
    "\n",
    "We train the model with only one batch. This means the model only ever sees the same `batch_size` number of images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-76e81732d77174f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: What do you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.seed_everything(123)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    devices=\"auto\",\n",
    "    accelerator=\"auto\",\n",
    "    precision=\"32\",\n",
    "    fast_dev_run=False if DEV_RUNS == 0 else DEV_RUNS,\n",
    "    max_steps=100,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\"),\n",
    "    # this option limits the training set to one batch, disables shuffle\n",
    "    overfit_batches=1,\n",
    ")\n",
    "\n",
    "net = SmallCNN()\n",
    "model = Classifier(net)\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=16, shuffle=True, num_workers=5)\n",
    "trainer.fit(model, train_dataloaders=dl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Metrics:  {trainer.logged_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Did it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - (Over)Fit\n",
    "\n",
    "In this step we try to drive the trainings-loss as low as possible.\n",
    "\n",
    "**Model Selection and Initialization**\n",
    "\n",
    "- Pick a proven model: Start with a simple, well-established architecture (e.g., ResNet-50 for image classification) rather than creating complex, custom models.\n",
    "- Use Adam optimizer: Begin with Adam and a learning rate of 3e-4 for its forgiving nature with hyperparameters (or the PyTorch default value).\n",
    "\n",
    "**Gradual Complexity**\n",
    "\n",
    "- Add complexity incrementally: Integrate multiple signals or features into your classifier one at a time, ensuring each addition improves performance.\n",
    "\n",
    "**Learning Rate Management**\n",
    "\n",
    "- Avoid default learning rate decay: Be cautious with repurposed code and learning rate decay schedules. Initially, disable learning rate decay and maintain a constant learning rate, tuning it later in the project.\n",
    "    \n",
    "\n",
    "You can do the following:\n",
    "- implement your own model\n",
    "- use a pre-defined model\n",
    "- use a pre-trained model\n",
    "\n",
    "**Important**: Inspect how the model performs. Which samples does it correctly classify? Which samples are wrong? Do you see a pattern? Can this be fixes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "We load the data again and start from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_cv_lectures import utils\n",
    "from dl_cv_lectures.data.image_folder import ImageFolder\n",
    "\n",
    "image_root_path = DATA_PATH.joinpath(\"cats_vs_dogs/PetImages\")\n",
    "\n",
    "ds = ImageFolder(image_root_path)\n",
    "\n",
    "all_ids = list(range(0, len(ds.observations)))\n",
    "all_labels = [x[\"label\"] for x in ds.observations]\n",
    "\n",
    "train_ids, val_ids, test_ids = utils.create_train_test_split(\n",
    "    all_ids, all_labels, random_state=123, test_size=0.2, val_size=0.1\n",
    ")\n",
    "\n",
    "\n",
    "ds_train = ImageFolder.from_subset(ds, train_ids)\n",
    "ds_val = ImageFolder.from_subset(ds, val_ids)\n",
    "ds_test = ImageFolder.from_subset(ds, test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1f3c0de79c9f603b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Pre-Trained Model\n",
    "\n",
    "In the following we will use a pre-trained model and adapt it to our dataset (transfer-learning).\n",
    "\n",
    "### Load Model\n",
    "\n",
    "Here we use a pre-trained model.  Read the doc here: [https://pytorch.org/vision/0.8/models.html](https://pytorch.org/vision/0.8/models.html).)\n",
    "\n",
    "**It is important to read how the data is pre-processed for a given pre-trained model. This should be consistent with how you pre-process the data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-39c22ca50688c3fe",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "net = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "tr_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.v2.RGB(),\n",
    "        transforms.RandomResizedCrop((128, 128)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tr_val = transforms.Compose(\n",
    "    [\n",
    "        transforms.v2.RGB(),\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "print(torchinfo.summary(net, input_size=(1, 3, 64, 64)))\n",
    "\n",
    "\n",
    "# if TRAIN:\n",
    "#     model.fit()\n",
    "# else:\n",
    "#     pass\n",
    "\n",
    "model.load_from_checkpoint()\n",
    "y = model.predict()\n",
    "\n",
    "# make_figure(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we adapt the output layer to match our dataset.\n",
    "\n",
    "**Adapt** to the correct number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fc = nn.Sequential(nn.Linear(512, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model. \n",
    "\n",
    "We also use a `logger` object to log the training process.\n",
    "\n",
    "Again: Adjust the parameters of the trainer class to your liking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_cv_lectures.data.image_folder import DataSetModule\n",
    "\n",
    "L.seed_everything(123)\n",
    "\n",
    "\n",
    "logger = TensorBoardLogger(\n",
    "    DATA_PATH.joinpath(\"lightning_logs\"), name=\"overfit_baseline1\"\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    devices=\"auto\",\n",
    "    accelerator=\"auto\",\n",
    "    precision=\"32\",\n",
    "    max_steps=500,\n",
    "    fast_dev_run=False if DEV_RUNS == 0 else DEV_RUNS,\n",
    "    enable_checkpointing=False,\n",
    "    logger=logger if not DEBUG else None,\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\"),\n",
    ")\n",
    "\n",
    "model = Classifier(net)\n",
    "\n",
    "\n",
    "dm = DataSetModule(\n",
    "    ds_train=ds_train,\n",
    "    ds_val=ds_val,\n",
    "    ds_test=ds_test,\n",
    "    classes=[\"Cat\", \"Dog\"],\n",
    "    train_transform=tr_train,\n",
    "    test_transform=tr_val,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Metrics:  {trainer.logged_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the tensorboard logs. This may not work in a container without opening tensorboard ports.\n",
    "\n",
    "(You would need to add the following options to docker run `-p 6006-6015:6006-6015`)\n",
    "\n",
    "If working locally, you can simply type: `localhost:6008' in your browser, after executing he following lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir={DATA_PATH.joinpath(\"lightning_logs\")} --host 0.0.0.0 --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Feel Free to try a larger model and observe the performance. You also might want to train your model longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: It is important to insepct model performance at this point. Try to figure out where the model works well and where it fails. And try to figure out why and if you can do something about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4) - Regularization\n",
    "\n",
    "Regularization is a process to deliberately limit a model's capacity in order to reduce overfitting and to improve generalization.\n",
    "\n",
    "**Data Collection and Augmentation**\n",
    "\n",
    "- Get more data: Collect additional real training data for the most effective regularization.\n",
    "- Data augmentation: Use more aggressive data augmentation techniques.\n",
    "- Creative augmentation: Explore simulation, hybrid methods, or GANs to expand datasets.\n",
    "\n",
    "**Model Initialization and Size**\n",
    "\n",
    "- Pretrain: Utilize pretrained networks when possible.\n",
    "- Smaller input dimensionality: Remove features with spurious signals and reduce image size if low-level details are not critical.\n",
    "- Smaller model size: Use domain knowledge to constrain and reduce the size of the network.\n",
    "\n",
    "**Regularization Techniques**\n",
    "\n",
    "- Decrease batch size: Smaller batch sizes can act as stronger regularizers due to batch normalization effects.\n",
    "- Add dropout: Use dropout (including dropout2d for ConvNets) sparingly.\n",
    "- Weight decay: Increase the weight decay penalty.\n",
    "- Early stopping: Stop training based on validation loss to avoid overfitting.\n",
    "\n",
    "**Model Complexity**\n",
    "\n",
    "- Try a larger model: Consider larger models for potentially better early-stopped performance, despite higher risk of eventual overfitting.\n",
    "\n",
    "  \n",
    "\n",
    "You can try the following techniques:\n",
    "\n",
    "- Weight Decay\n",
    "- Data Augmentation\n",
    "- Early Stopping on Validation Set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Decay\n",
    "\n",
    "Weight decay is a technique to reduce model complexity by adding a penalty to the magnitude of the weights. It can be implemented by decaying the weights towards 0 after each gradient descent step. \n",
    "\n",
    "Read the following documentation and add Weight Decay to your model: [torch.optim.Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)\n",
    "\n",
    "It is implemented in the optimizer.\n",
    "\n",
    "Make it configurable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1df2c55234f1bd22",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Classifier(L.LightningModule):\n",
    "    def __init__(self, model, weight_decay: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.weight_decay = weight_decay\n",
    "        self.train_accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.train_loss = torchmetrics.MeanMetric()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch[\"image\"], batch[\"label\"]\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # Update accuracy metric\n",
    "        self.train_accuracy(preds, y)\n",
    "        self.train_loss(loss)\n",
    "\n",
    "        self.log(\"train_acc_step\", self.train_accuracy, prog_bar=True)\n",
    "        self.log(\"train_loss_step\", self.train_loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        # log epoch metric\n",
    "        self.log(\"train_acc_epoch\", self.train_accuracy)\n",
    "        self.log(\"train_loss_epoch\", self.train_loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(\n",
    "            self.parameters(), lr=0.001, weight_decay=self.weight_decay\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Data augmentation is the process of applying random transformations to the input data before it is processed by the model. This increases the robustness of the model and improves its generalization capabilities.\n",
    "\n",
    "**Note**: Always check if the data augmentations are plausible and not too extreme!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.v2.RGB(),\n",
    "        transforms.RandomResizedCrop((128, 128)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tr_val = transforms.Compose(\n",
    "    [\n",
    "        transforms.v2.RGB(),\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ds_train = ImageFolder.from_subset(ds, train_ids, transform=tr_train)\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=16, shuffle=True, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = next(iter(dl_train))\n",
    "\n",
    "ts.show(obs[\"image\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Does the data still look plausible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.seed_everything(123)\n",
    "\n",
    "logger = TensorBoardLogger(\n",
    "    DATA_PATH.joinpath(\"lightning_logs\"), name=\"data_augmentation\"\n",
    ")\n",
    "\n",
    "model = Classifier(net)\n",
    "trainer = L.Trainer(\n",
    "    devices=\"auto\",\n",
    "    accelerator=\"auto\",\n",
    "    precision=\"32\",\n",
    "    max_steps=500,\n",
    "    fast_dev_run=DEV_RUNS if DEV_RUNS > 0 else False,\n",
    "    enable_checkpointing=False,\n",
    "    logger=logger if not DEBUG else None,\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\"),\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloaders=dl_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "Early stopping monitors the training process on a separate validation set to determine the optimal point regarding when to stop training (when validation loss / metric is at the best level).\n",
    "\n",
    "Pytorch-lightning provides such functionality out-of-the-box: [pytorch_lightning.callbacks.early_stopping.EarlyStopping](https://lightning.ai/docs/pytorch/stable/common/early_stopping.html)\n",
    "\n",
    "**Make sure to let the model run enough steps such that early stopping is actually stopping the training!**\n",
    "\n",
    "Implement a metric which early stopping should monitor. It should be one calculated on the validation set.\n",
    "\n",
    "\n",
    "Inspect the `Trainer` class and set more appropriate values  (e.g. `val_check_interval` and `max_steps`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1f63d38b919c30a9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val/accuracy_epoch\", min_delta=0.00, patience=3, mode=\"max\", verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a more fully fledged class of a classifier lightning module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.seed_everything(123)\n",
    "\n",
    "from dl_cv_lectures import classifier\n",
    "\n",
    "logger = TensorBoardLogger(DATA_PATH.joinpath(\"lightning_logs\"), name=\"early_stopping\")\n",
    "\n",
    "model = classifier.Classifier(net, num_classes=2)\n",
    "trainer = L.Trainer(\n",
    "    devices=\"auto\",\n",
    "    accelerator=\"auto\",\n",
    "    precision=\"32\",\n",
    "    max_epochs=2,\n",
    "    fast_dev_run=False if DEV_RUNS == 0 else DEV_RUNS,\n",
    "    enable_checkpointing=False,\n",
    "    logger=logger if not DEBUG else None,\n",
    "    callbacks=[early_stopping],  # Add the early stopping callback here\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\"),\n",
    ")\n",
    "\n",
    "dl_val = DataLoader(ds_val, batch_size=64, shuffle=False, num_workers=5)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=dl_train, val_dataloaders=dl_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Compare training metrics with validation metrics. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Hyper-Parameter Optimization\n",
    "\n",
    "To optimize hyper parameters we need to consider the following:\n",
    "- paramaterize training process (architecture and pre-processing)\n",
    "- experiment tracking software\n",
    "- evaluation procedures (such as cross-validation for smaller datasets)\n",
    "- use specialized packages to track and run experiments in parallel\n",
    "\n",
    "\n",
    "**Hyper-Parameter Tuning can be time consuming!** You might want to skip it for now.\n",
    "\n",
    "Ideally one uses special libraries such as [RayTune](https://docs.ray.io/en/latest/tune/index.html).\n",
    "\n",
    "We combine `RayTune` with `Lightning`. Here is the example we follow below: [Link](https://docs.ray.io/en/latest/tune/examples/tune-pytorch-lightning.html)\n",
    "\n",
    "We systematically test different data augmentation methods.\n",
    "\n",
    "We use a `L.LightningDataModule` to parameterize and simplify data preparation, since data augmentation is defined on the dataset level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We setup all the necessary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_cv_lectures import utils\n",
    "from dl_cv_lectures.data.image_folder import ImageFolder\n",
    "\n",
    "image_root_path = DATA_PATH.joinpath(\"cats_vs_dogs/PetImages\")\n",
    "\n",
    "ds = ImageFolder(image_root_path)\n",
    "\n",
    "all_ids = list(range(0, len(ds.observations)))\n",
    "all_labels = [x[\"label\"] for x in ds.observations]\n",
    "\n",
    "train_ids, val_ids, test_ids = utils.create_train_test_split(\n",
    "    all_ids, all_labels, random_state=123, test_size=0.2, val_size=0.1\n",
    ")\n",
    "\n",
    "ds_train = ImageFolder.from_subset(ds, train_ids)\n",
    "ds_val = ImageFolder.from_subset(ds, val_ids)\n",
    "ds_test = ImageFolder.from_subset(ds, test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameter Grid\n",
    "\n",
    "We need to define a `search_space` that specifies which hyper parameters to vary and how to sample from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the different augmentation strategies\n",
    "norm = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.v2.RGB(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        norm,\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "simple_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.v2.RGB(),\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        norm,\n",
    "    ]\n",
    ")\n",
    "\n",
    "complex_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.v2.RGB(),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        norm,\n",
    "    ]\n",
    ")\n",
    "\n",
    "wild_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.v2.RGB(),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(degrees=20),\n",
    "        transforms.RandomApply(\n",
    "            [\n",
    "                transforms.ColorJitter(\n",
    "                    brightness=0.5, contrast=0.5, saturation=0.5, hue=0.2\n",
    "                )\n",
    "            ],\n",
    "            p=0.8,\n",
    "        ),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        norm,\n",
    "    ]\n",
    ")\n",
    "\n",
    "PRE_PROCESSING = {\n",
    "    \"test\": test_transforms,\n",
    "    \"simple\": simple_transforms,\n",
    "    \"complex\": complex_transforms,\n",
    "    \"wild\": wild_transforms,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.init(configure_logging=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the search space we define. We only want to vary the data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "search_space = {\n",
    "    \"pre_processing\": tune.grid_search([\"simple\", \"complex\", \"wild\"]),\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DataSetModule(\n",
    "    ds_train=ds_train,\n",
    "    ds_val=ds_val,\n",
    "    ds_test=ds_test,\n",
    "    classes=[\"Cat\", \"Dog\"],\n",
    "    train_transform=complex_transforms,\n",
    "    test_transform=test_transforms,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = dm.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_val = dm.val_dataloader()\n",
    "batch = next(iter(dl_val))\n",
    "batch[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Ray Tuner Loop\n",
    "\n",
    "Next we define a configurable `train_func` which creates a `lightning.Trainer` object with some additional `ray.tuner` magic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1bb9b324c9f0b92d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from ray.train.lightning import (\n",
    "    RayDDPStrategy,\n",
    "    RayLightningEnvironment,\n",
    "    RayTrainReportCallback,\n",
    "    prepare_trainer,\n",
    ")\n",
    "\n",
    "from dl_cv_lectures import classifier\n",
    "from dl_cv_lectures.data.image_folder import DataSetModule\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "\n",
    "def train_func(config):\n",
    "    dm = DataSetModule(\n",
    "        ds_train=ds_train,\n",
    "        ds_val=ds_val,\n",
    "        ds_test=ds_test,\n",
    "        classes=[\"Cat\", \"Dog\"],\n",
    "        train_transform=PRE_PROCESSING[config[\"pre_processing\"]],\n",
    "        test_transform=test_transforms,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "    )\n",
    "\n",
    "    logger = TensorBoardLogger(DATA_PATH.joinpath(\"lightning_logs\"), name=\"ray_tune\")\n",
    "\n",
    "    net = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    net.fc = nn.Sequential(nn.Linear(512, NUM_CLASSES))\n",
    "\n",
    "    model = classifier.Classifier(net, num_classes=2)\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        devices=\"auto\",\n",
    "        accelerator=\"auto\",\n",
    "        precision=\"32\",\n",
    "        max_epochs=config[\"epochs\"],\n",
    "        fast_dev_run=False if DEV_RUNS == 0 else DEV_RUNS,\n",
    "        enable_checkpointing=False,\n",
    "        logger=logger if not DEBUG else None,\n",
    "        # callbacks=[early_stopping],  # Add the early stopping callback here\n",
    "        # default_root_dir=DATA_PATH.joinpath(\"lightning_logs\"),\n",
    "        strategy=RayDDPStrategy(),\n",
    "        callbacks=[RayTrainReportCallback()],\n",
    "        plugins=[RayLightningEnvironment()],\n",
    "        enable_progress_bar=False,\n",
    "    )\n",
    "\n",
    "    trainer = prepare_trainer(trainer)\n",
    "    trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify the hardware ressources that we want to invest in the tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train import CheckpointConfig, RunConfig, ScalingConfig\n",
    "\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=1, use_gpu=True, resources_per_worker={\"CPU\": 4, \"GPU\": 1}\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    checkpoint_config=CheckpointConfig(\n",
    "        num_to_keep=None,\n",
    "        checkpoint_score_attribute=\"val/accuracy_epoch\",\n",
    "        checkpoint_score_order=\"max\",\n",
    "    ),\n",
    "    storage_path=DATA_PATH.joinpath(\"lightning_logs/ray\"),\n",
    "    name=\"data_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "# Define a TorchTrainer without hyper-parameters for Tuner\n",
    "ray_trainer = TorchTrainer(\n",
    "    train_func,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a scheduler, which is responible to select parameters for model training.\n",
    "\n",
    "In this case we deliberately only have 3 different hyper-parameter options. But it could be much much more from which the scheduler must choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "\n",
    "def tune_asha(num_samples=10):\n",
    "    scheduler = ASHAScheduler(\n",
    "        time_attr=\"training_iteration\", max_t=1000, grace_period=100, reduction_factor=2\n",
    "    )\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        ray_trainer,\n",
    "        param_space={\"train_loop_config\": search_space},\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"val/accuracy_epoch\",\n",
    "            mode=\"max\",\n",
    "            num_samples=num_samples,\n",
    "            scheduler=scheduler,\n",
    "        ),\n",
    "    )\n",
    "    return tuner.fit()\n",
    "\n",
    "\n",
    "if not DEBUG:\n",
    "    results = tune_asha(num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DEBUG:\n",
    "    results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 - Squeeze out the Juice!\n",
    "\n",
    "You can try the following techniques to get even further:\n",
    "\n",
    "- advanced data augmentation. For example: https://pytorch.org/vision/main/auto_examples/transforms/plot_cutmix_mixup.html#sphx-glr-auto-examples-transforms-plot-cutmix-mixup-py\n",
    "- model ensembling. Train multiple models and combine their predictions.\n",
    "- advanced techniques: AdamW Optimizer, Stochastic Depth Regularization\n",
    "\n",
    "\n",
    "**Task**: Try it out for yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-52424cc5813975bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# 3) Evaluate your model\n",
    "\n",
    "We may want to evaluate our model in more detail. In particular we want to know where the model works well and where it fails. This might give us additional insight in the data and the difficulties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_cv_lectures import utils\n",
    "from dl_cv_lectures.data.image_folder import ImageFolder\n",
    "\n",
    "image_root_path = DATA_PATH.joinpath(\"cats_vs_dogs/PetImages\")\n",
    "\n",
    "ds = ImageFolder(image_root_path)\n",
    "\n",
    "all_ids = list(range(0, len(ds.observations)))\n",
    "all_labels = [x[\"label\"] for x in ds.observations]\n",
    "\n",
    "train_ids, val_ids, test_ids = utils.create_train_test_split(\n",
    "    all_ids, all_labels, random_state=123, test_size=0.2, val_size=0.1\n",
    ")\n",
    "\n",
    "ds_train = ImageFolder.from_subset(ds, train_ids)\n",
    "ds_val = ImageFolder.from_subset(ds, val_ids)\n",
    "ds_test = ImageFolder.from_subset(ds, test_ids)\n",
    "\n",
    "\n",
    "dm = DataSetModule(\n",
    "    ds_train=ds_train,\n",
    "    ds_val=ds_val,\n",
    "    ds_test=ds_test,\n",
    "    classes=[\"Cat\", \"Dog\"],\n",
    "    train_transform=complex_transforms,\n",
    "    test_transform=test_transforms,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    devices=\"auto\",\n",
    "    accelerator=\"auto\",\n",
    "    precision=\"32\",\n",
    "    max_epochs=2,\n",
    "    fast_dev_run=False if DEV_RUNS == 0 else DEV_RUNS,\n",
    "    enable_checkpointing=False,\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\"),\n",
    ")\n",
    "\n",
    "# trainer.test(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3368f29ef9e4cc98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Confusion-Matrix\n",
    "\n",
    "Plotten Sie eine _confusion matrix_. Benutzen Sie \n",
    "\n",
    "- [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "- [ConfusionMatrixDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-78b9e4f3c69747d5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(8, 8))\n",
    "# cm = confusion_matrix(y_true=true_all, y_pred=predicted_all)\n",
    "# disp = ConfusionMatrixDisplay(cm, display_labels=classes)\n",
    "# disp.plot(ax=ax, xticks_rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Which classes are confused how?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
