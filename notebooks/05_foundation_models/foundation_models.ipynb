{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Foundation Models\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Download, setup foundation model\n",
    "- Perform zero-shot image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9856bbf98ea570f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Google Colab Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Detect Colab\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "print(f\"In Colab: {IN_COLAB}\")\n",
    "\n",
    "# Show prominent message if in Colab\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        from IPython.display import Markdown, display\n",
    "\n",
    "        display(\n",
    "            Markdown(\n",
    "                \"\"\"\n",
    "> üíæ **Optionally:**  \n",
    "> Save this notebook to your **personal Google Drive** to persist any changes.\n",
    ">\n",
    "> *Go to `File ‚ñ∏ Save a copy in Drive` before editing.*\n",
    "            \"\"\"\n",
    "            )\n",
    "        )\n",
    "    except Exception:\n",
    "        print(\n",
    "            \"\\nüíæ Optionally: Save the notebook to your personal Google Drive to persist changes.\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mount google drive to store data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Data Path\n",
    "\n",
    "**Modify the following paths if necessary.**\n",
    "\n",
    "That is where your data will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if IN_COLAB:\n",
    "    DATA_PATH = Path(\"/content/drive/MyDrive/cas-dl-module-compvis-part1\")\n",
    "else:\n",
    "    DATA_PATH = Path(\"../../data\")\n",
    "assert DATA_PATH.exists(), f\"PATH: {DATA_PATH} does not exist.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Lectures Package\n",
    "\n",
    "Install `dl_cv_lectures` package with all necessary dependencies.\n",
    "\n",
    "This package provides the environment of the exercises-repository, as well as helper- and utils modules: [Link](https://github.com/marco-willi/cas-dl-compvis-exercises-hs2025)\n",
    "\n",
    "The following code installs the package from a local repository (if available), otherwise it installs it from the exercise repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "\n",
    "def ensure_dl_cv_lectures():\n",
    "    \"\"\"Ensure dl_cv_lectures is installed (local or from GitHub).\"\"\"\n",
    "    try:\n",
    "        import dl_cv_lectures\n",
    "\n",
    "        console.print(\n",
    "            \"[bold green]‚úÖ dl_cv_lectures installed ‚Äî all good![/bold green]\"\n",
    "        )\n",
    "        return\n",
    "    except ImportError:\n",
    "        console.print(\"[bold yellow]‚ö†Ô∏è dl_cv_lectures not found.[/bold yellow]\")\n",
    "    repo_path = Path(\"/workspace/pyproject.toml\")\n",
    "    if repo_path.exists():\n",
    "        console.print(\"[cyan]üì¶ Installing from local repository...[/cyan]\")\n",
    "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \"/workspace\"]\n",
    "    else:\n",
    "        console.print(\"[cyan]üåê Installing from GitHub repository...[/cyan]\")\n",
    "        cmd = [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"git+https://github.com/marco-willi/cas-dl-compvis-exercises-hs2025\",\n",
    "        ]\n",
    "    try:\n",
    "        subprocess.run(cmd, check=True)\n",
    "        console.print(\"[bold green]‚úÖ Installation successful![/bold green]\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        console.print(f\"[bold red]‚ùå Installation failed ({e}).[/bold red]\")\n",
    "\n",
    "\n",
    "ensure_dl_cv_lectures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries\n",
    "\n",
    "Load all libraries and packages used in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a default device for your computations.\n",
    "\n",
    "**GPU is strongly recommended!** (otherwise the images have to be restricted in size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)  The CLIP Model\n",
    "\n",
    "The CLIP model [Link](https://arxiv.org/abs/2103.00020) has had a profound impact in the deep learning community and in practical applications.\n",
    "\n",
    "We are going to use it for zero-shot image classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "model = CLIPModel.from_pretrained(\n",
    "    \"openai/clip-vit-base-patch32\", cache_dir=DATA_PATH.joinpath(\"hf_cache\")\n",
    ")\n",
    "processor = CLIPProcessor.from_pretrained(\n",
    "    \"openai/clip-vit-base-patch32\", cache_dir=DATA_PATH.joinpath(\"hf_cache\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a prompt for each class that we are interested in. In this example the classes `cat` and `dog`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text=[\"a photo of a cat\", \"a photo of a dog\"],\n",
    "    images=image,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ")\n",
    "# we can take the softmax to get the label probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we evaluate the similarities of the image with respect to each prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the relative similarities and produce a probability distribution (softmax) over all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = logits_per_image.softmax(dim=1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Play around with the prompts. Can you also classify / detect other objects in the images?  How about a different image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) DINOv3: Self-Supervised Image Embeddings\n",
    "\n",
    "DINOv3 is a state-of-the-art self-supervised vision transformer that learns powerful visual representations without requiring labels. \n",
    "\n",
    "In this exercise, we'll:\n",
    "1. **Extract and compare image embeddings** - Compare global features between different images\n",
    "2. **Analyze patch-based embeddings** - Explore local features within images to understand spatial relationships\n",
    "\n",
    "**What makes DINOv3 special?**\n",
    "- Trained using self-distillation (DINO = **D**istillation with **NO** labels)\n",
    "- Produces both global (image-level) and local (patch-level) features\n",
    "- Excellent transfer learning capabilities for downstream tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DINOv3 Model with timm\n",
    "\n",
    "We'll use the `timm` library which provides easy access to the DINOv3 model trained on the LVD-1689M dataset (1.689 billion images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import timm\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Load DINOv3 ViT-Base model from timm\n",
    "model_name = \"vit_base_patch16_dinov3.lvd1689m\"\n",
    "\n",
    "dinov3 = timm.create_model(\n",
    "    model_name,\n",
    "    pretrained=True,\n",
    "    num_classes=0,  # Remove classification head to get embeddings\n",
    ")\n",
    "dinov3 = dinov3.to(device)\n",
    "dinov3.eval()\n",
    "\n",
    "# Get model-specific transforms\n",
    "data_config = timm.data.resolve_model_data_config(dinov3)\n",
    "transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {model_name}\")\n",
    "print(\"üìä Model stats:\")\n",
    "print(f\"   - Parameters: {sum(p.numel() for p in dinov3.parameters()) / 1e6:.1f}M\")\n",
    "print(f\"   - Input size: {data_config['input_size']}\")\n",
    "print(f\"   - Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data\n",
    "\n",
    "Now we get some images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_cv_lectures.data import (\n",
    "    cats_vs_dogs,\n",
    ")\n",
    "from dl_cv_lectures.data.image_folder import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_vs_dogs.download(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_root_path = DATA_PATH.joinpath(\"cats_vs_dogs/PetImages\")\n",
    "ds = ImageFolder(image_root_path)\n",
    "ds.classes\n",
    "ds[0]\n",
    "ds[0][\"image\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's calculate Image Embeddings\n",
    "\n",
    "\n",
    "Let's select a random number of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Select random images\n",
    "n_samples = 200  # Number of images to sample\n",
    "random.seed(123)  # For reproducibility\n",
    "\n",
    "# Get random indices\n",
    "random_indices = random.sample(range(len(ds)), min(n_samples, len(ds)))\n",
    "\n",
    "# Get the sampled images\n",
    "sampled_images = [ds[i] for i in random_indices]\n",
    "\n",
    "sampled_labels = [img[\"label\"] for img in sampled_images]\n",
    "\n",
    "print(f\"Selected {len(sampled_images)} random images\")\n",
    "print(f\"Classes: {[img['label'] for img in sampled_images[:5]]}\")  # Show first 5 labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calcualate the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate global embeddings for all sampled images\n",
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sample in sampled_images:\n",
    "        # Preprocess image\n",
    "        img_tensor = transforms(sample[\"image\"]).unsqueeze(0).to(device)\n",
    "\n",
    "        # Get global embedding\n",
    "        embedding = dinov3(img_tensor)\n",
    "\n",
    "        embeddings.append(embedding.cpu())\n",
    "        labels.append(sample[\"label\"])\n",
    "\n",
    "# Stack all embeddings into a single tensor\n",
    "embeddings = torch.cat(embeddings, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "print(\"‚úÖ Embeddings calculated!\")\n",
    "print(f\"   - Shape: {embeddings.shape}\")\n",
    "print(f\"   - Labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Calculate t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=123, perplexity=30, max_iter=1000)\n",
    "embeddings_2d = tsne.fit_transform(embeddings.numpy())\n",
    "\n",
    "print(\"‚úÖ t-SNE calculated!\")\n",
    "print(f\"   - Original shape: {embeddings.shape}\")\n",
    "print(f\"   - Reduced shape: {embeddings_2d.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect how the images cluster. Let's use tsne clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "_ = sns.scatterplot(\n",
    "    x=embeddings_2d[:, 0], y=embeddings_2d[:, 1], alpha=0.7, s=50, ax=ax\n",
    ").set(\n",
    "    title=\"t-SNE visualization of DINOv3 Image Embeddings\",\n",
    "    xlabel=\"t-SNE dimension 1\",\n",
    "    ylabel=\"t-SNE dimension 2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "_ = sns.scatterplot(\n",
    "    x=embeddings_2d[:, 0],\n",
    "    y=embeddings_2d[:, 1],\n",
    "    hue=sampled_labels,\n",
    "    alpha=0.7,\n",
    "    s=50,\n",
    "    ax=ax,\n",
    ").set(\n",
    "    title=\"t-SNE visualization of DINOv3 Image Embeddings\",\n",
    "    xlabel=\"t-SNE dimension 1\",\n",
    "    ylabel=\"t-SNE dimension 2\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
