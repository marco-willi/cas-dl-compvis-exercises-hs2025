{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Convolutions: Apply on images\n",
    "- CNNs: Define, Optimize, Inspect, Understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9856bbf98ea570f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Google Colab Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Detect Colab\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "print(f\"In Colab: {IN_COLAB}\")\n",
    "\n",
    "# Show prominent message if in Colab\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        from IPython.display import Markdown, display\n",
    "\n",
    "        display(\n",
    "            Markdown(\n",
    "                \"\"\"\n",
    "> üíæ **Optionally:**  \n",
    "> Save this notebook to your **personal Google Drive** to persist any changes.\n",
    ">\n",
    "> *Go to `File ‚ñ∏ Save a copy in Drive` before editing.*\n",
    "            \"\"\"\n",
    "            )\n",
    "        )\n",
    "    except Exception:\n",
    "        print(\n",
    "            \"\\nüíæ Optionally: Save the notebook to your personal Google Drive to persist changes.\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mount google drive to store data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Data Path\n",
    "\n",
    "**Modify the following paths if necessary.**\n",
    "\n",
    "That is where your data will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if IN_COLAB:\n",
    "    DATA_PATH = Path(\"/content/drive/MyDrive/cas-dl-module-compvis-part1\")\n",
    "else:\n",
    "    DATA_PATH = Path(\"../../data\")\n",
    "assert DATA_PATH.exists(), f\"PATH: {DATA_PATH} does not exist.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Lectures Package\n",
    "\n",
    "Install `dl_cv_lectures` package with all necessary dependencies.\n",
    "\n",
    "This package provides the environment of the exercises-repository, as well as helper- and utils modules: [Link](https://github.com/marco-willi/cas-dl-compvis-exercises-hs2025)\n",
    "\n",
    "The following code installs the package from a local repository (if available), otherwise it installs it from the exercise repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "\n",
    "def ensure_dl_cv_lectures():\n",
    "    \"\"\"Ensure dl_cv_lectures is installed (local or from GitHub).\"\"\"\n",
    "    try:\n",
    "        import dl_cv_lectures\n",
    "\n",
    "        console.print(\n",
    "            \"[bold green]‚úÖ dl_cv_lectures installed ‚Äî all good![/bold green]\"\n",
    "        )\n",
    "        return\n",
    "    except ImportError:\n",
    "        console.print(\"[bold yellow]‚ö†Ô∏è dl_cv_lectures not found.[/bold yellow]\")\n",
    "    repo_path = Path(\"/workspace/pyproject.toml\")\n",
    "    if repo_path.exists():\n",
    "        console.print(\"[cyan]üì¶ Installing from local repository...[/cyan]\")\n",
    "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \"/workspace\"]\n",
    "    else:\n",
    "        console.print(\"[cyan]üåê Installing from GitHub repository...[/cyan]\")\n",
    "        cmd = [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"git+https://github.com/marco-willi/cas-dl-compvis-exercises-hs2025\",\n",
    "        ]\n",
    "    try:\n",
    "        subprocess.run(cmd, check=True)\n",
    "        console.print(\"[bold green]‚úÖ Installation successful![/bold green]\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        console.print(f\"[bold red]‚ùå Installation failed ({e}).[/bold red]\")\n",
    "\n",
    "\n",
    "ensure_dl_cv_lectures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries\n",
    "\n",
    "Load all libraries and packages used in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from collections.abc import Callable\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchinfo\n",
    "import torchshow as ts\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision.transforms.v2 import functional as TF\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dl_cv_lectures.transform import RandomQuadrantPad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a default device for your computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Convolutions in PyTorch\n",
    "\n",
    "We apply a _convolution_ on images.\n",
    "\n",
    "Let's get an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/pytorch/vision/blob/main/gallery/assets/dog2.jpg?raw=true\"\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "image = Image.open(io.BytesIO(r.content))\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a kernel / filter by hand. Let's take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define kernel by hand\n",
    "conv_kernel = (\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [[1, 0, -1], [1, 0, -1], [1, 0, -1]],  # R\n",
    "            [[1, 0, -1], [1, 0, -1], [1, 0, -1]],  # G\n",
    "            [[1, 0, -1], [1, 0, -1], [1, 0, -1]],  # B\n",
    "        ]\n",
    "    )\n",
    "    .unsqueeze(0)\n",
    "    .float()\n",
    ")\n",
    "\n",
    "\n",
    "ts.show(conv_kernel, show_axis=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the kernel to the image. We need to convert the image to a `torch.tensor` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = TF.pil_to_tensor(image).float() / 255.0\n",
    "\n",
    "activations = F.conv2d(\n",
    "    image_tensor, conv_kernel, stride=1, padding=0, dilation=1, groups=1\n",
    ")\n",
    "\n",
    "# rescale activations to visualize them as an image\n",
    "activations_scaled = (activations - activations.min()) / (\n",
    "    activations.max() - activations.min()\n",
    ")\n",
    "\n",
    "ts.show(activations_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the activations to the input image. What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(18, 6), ncols=3)\n",
    "# activations\n",
    "ax = axes[0]\n",
    "img = TF.to_pil_image(activations_scaled)\n",
    "img_ax = ax.imshow(img)\n",
    "_ = ax.axis(\"off\")\n",
    "cbar = fig.colorbar(img_ax, ax=ax, orientation=\"horizontal\", fraction=0.046, pad=0.04)\n",
    "cbar.set_label(\"Activation value\", fontsize=10)\n",
    "# activations - center crop\n",
    "img_crop = TF.center_crop(activations_scaled, output_size=(128, 128))\n",
    "img = TF.to_pil_image(img_crop)\n",
    "ax = axes[1]\n",
    "img_ax = ax.imshow(img)\n",
    "_ = ax.axis(\"off\")\n",
    "cbar = fig.colorbar(img_ax, ax=ax, orientation=\"horizontal\", fraction=0.046, pad=0.04)\n",
    "cbar.set_label(\"Activation value\", fontsize=10)\n",
    "# original image\n",
    "ax = axes[2]\n",
    "img_crop = TF.center_crop(TF.pil_to_tensor(image), output_size=(128, 128))\n",
    "img = TF.to_pil_image(img_crop)\n",
    "img_ax = ax.imshow(img)\n",
    "_ = ax.axis(\"off\")\n",
    "dummy_cbar = fig.colorbar(\n",
    "    img_ax, ax=ax, orientation=\"horizontal\", fraction=0.046, pad=0.04\n",
    ")\n",
    "dummy_cbar.ax.set_visible(False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What did the convolutional kernel do / what image features lead to high activations?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "A simple way to interpret a filter is to visually inspect it.\n",
    "\n",
    "Our filter is $3x3$ in size with columns of identical values, from left to right:\n",
    "\n",
    "- high value\n",
    "\n",
    "- zero value\n",
    "\n",
    "- negative value\n",
    "\n",
    "\n",
    "On an image with values in the range of 0 and 255 the output activations will be highest when the input activations are high (bright) for the first column, irrelevant for the second, and low (dark) on the third column. This means strong vertical edges from left (bright) to right (dark) will activate strongly.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two approaches to apply operations in PyTorch:\n",
    "\n",
    "- *Functional*: These are *stateless* functions [nn.functional](https://pytorch.org/docs/stable/nn.functional.html)\n",
    "- *Modules*: Using *stateful* objects which are used in neural networks [torch.nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform a *convolution* using a *module*. [torch.nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module\n",
    "conv_module = torch.nn.Conv2d(\n",
    "    in_channels=3,\n",
    "    out_channels=1,\n",
    "    kernel_size=3,\n",
    "    stride=1,\n",
    "    padding=0,\n",
    "    dilation=1,\n",
    "    groups=1,\n",
    ")\n",
    "\n",
    "activations = conv_module(image_tensor)\n",
    "\n",
    "# rescale result to visualize it as an image\n",
    "activations = (activations - activations.min()) / (\n",
    "    activations.max() - activations.min()\n",
    ")\n",
    "\n",
    "\n",
    "ts.show(activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dfdbcda98b2ef9b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: What is the difference between the *functional* and *module* approach? What happens in the *module* approach?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Functional approach (`torch.nn.functional`):**\n",
    "- Functions that perform operations **stateless**ly\n",
    "- No parameters are stored/maintained\n",
    "- You need to pass parameters (like filters/weights) explicitly each time\n",
    "- Example: `F.conv2d(input, weight, ...)`\n",
    "- More flexible but requires manual parameter management\n",
    "\n",
    "**Module approach (`torch.nn.Module`):**\n",
    "- Classes that **encapsulate both parameters and operations**\n",
    "- Parameters (weights, biases) are automatically registered and managed\n",
    "- Gradients are tracked automatically during backpropagation\n",
    "- Example: `nn.Conv2d(...)` creates a layer with learnable parameters\n",
    "- Parameters accessible via `.parameters()` method\n",
    "- Better for building neural networks with trainable weights\n",
    "\n",
    "**What happens in the module approach:**\n",
    "1. When you instantiate a module (e.g., `nn.Conv2d(1, 16, 5)`), it **initializes learnable parameters** (filters/weights and optionally biases)\n",
    "2. These parameters are registered as part of the module's state\n",
    "3. During forward pass, the module applies the operation using its stored parameters\n",
    "4. During training, optimizer can access and update these parameters via `.parameters()`\n",
    "\n",
    "**Use functional when:** You want to apply operations with custom/fixed parameters\n",
    "**Use module when:** Building neural networks with learnable parameters (most common case)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply the following operations to the image using the *functional* approach:\n",
    "\n",
    "- Convolution\n",
    "- Max Pooling\n",
    "- Convolution\n",
    "\n",
    "You can use the filter from above, if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f6d5d5c659007899",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x = F.conv2d(image_tensor, conv_kernel, stride=1, padding=0, dilation=1, groups=1)\n",
    "x = F.max_pool2d(x, kernel_size=(2, 2), stride=(2, 2))\n",
    "activations = F.conv2d(\n",
    "    x, conv_kernel[:, 0:1, :, :], stride=1, padding=0, dilation=1, groups=1\n",
    ")\n",
    "\n",
    "activations = (activations - activations.min()) / (\n",
    "    activations.max() - activations.min()\n",
    ")\n",
    "ts.show(activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) CNN Properties\n",
    "\n",
    "In the following we will conduct a few experiments to understand how CNNs work and to contrast them with MLPs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "We create a modified MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MNIST dataset with the custom transform\n",
    "ds_mnist_train = torchvision.datasets.MNIST(\n",
    "    root=DATA_PATH,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=RandomQuadrantPad(choices=[\"top_left\"]),\n",
    ")\n",
    "\n",
    "# Create the MNIST dataset with the custom transform\n",
    "ds_mnist_test_tl = torchvision.datasets.MNIST(\n",
    "    root=DATA_PATH,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=RandomQuadrantPad(choices=[\"top_left\"]),\n",
    ")\n",
    "\n",
    "# Create the MNIST dataset with the custom transform\n",
    "ds_mnist_test_br = torchvision.datasets.MNIST(\n",
    "    root=DATA_PATH,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=RandomQuadrantPad(choices=[\"bottom_right\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize a few data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_mnist_train = torch.utils.data.DataLoader(\n",
    "    ds_mnist_train, batch_size=12, shuffle=True, num_workers=4\n",
    ")\n",
    "\n",
    "# Let's check the first batch\n",
    "images, labels = next(iter(dl_mnist_train))\n",
    "\n",
    "ts.show(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-093dfa37667fda26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: What do you notice?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "- **Digits are positioned in the top left qudrant**\n",
    "- **Large black borders**: Most of the image is black/zero-padding with the actual digit occupying only 1/4 of the space\n",
    "- **Image size increased**: Original 28√ó28 MNIST images are now embedded in 56√ó56 images (2√ó in each dimension)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture Definition\n",
    "\n",
    "We define a CNN with the following architecture:\n",
    "\n",
    "- Input Shape: (1, 28 *  2, 28 *2)\n",
    "- Convolution1: 8 Filters, Kernel-Size 5x5\n",
    "- Max Pooling: Stride 2, Kernel-Size 2\n",
    "- Convolution2: 16 Filter, Kernel-Size 5x5\n",
    "- Max Pooling: Stride 2, Kernel-Size 2\n",
    "- FC1: 32 neurons\n",
    "- FC2: 16 neurons\n",
    "- FC3: 10 neurons (because we have 10 classes)\n",
    "\n",
    "We use ReLU after each layer and subclass `torch.nn.Module`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Implement the CNN as defined above in the next cell.\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_channel=8, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Feature extractor ---\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv block 1\n",
    "            nn.Conv2d(1, num_channel, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Conv block 2\n",
    "            nn.Conv2d(num_channel, num_channel * 2, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        # --- Classifier ---\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # 11 * 11 is the output - we can either specify explicitly \n",
    "            # or use nn.LazyLinear which infers the number of input\n",
    "            # features at runtime\n",
    "            #nn.Linear(num_channel * 2 * 11 * 11, num_channel * 4),\n",
    "            nn.LazyLinear(num_channel * 4)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_channel * 4, num_channel * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_channel * 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6c456bb869bd831b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_channel=8, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Feature extractor ---\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv block 1\n",
    "            nn.Conv2d(1, num_channel, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Conv block 2\n",
    "            nn.Conv2d(num_channel, num_channel * 2, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        # --- Classifier ---\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # 11 * 11 is the output\n",
    "            # nn.Linear(num_channel * 2 * 11 * 11, num_channel * 4),\n",
    "            nn.LazyLinear(num_channel * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_channel * 4, num_channel * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_channel * 2, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "\n",
    "print(net)\n",
    "print(torchinfo.summary(net, input_size=(1, 1, 56, 56)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Explain why layer FC1 is defined with `nn.Linear(num_channel*2 * 11 * 11, num_channel*4)`.\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "**Explanation of `nn.Linear(num_channel*2 * 11 * 11, num_channel*4)`:**\n",
    "\n",
    "This linear layer connects the flattened convolutional output to the first fully-connected layer. Let's break down why it's `11 √ó 11`:\n",
    "\n",
    "1. **Input image**: 56√ó56\n",
    "2. **After conv1 (5√ó5, no padding)**: (56 - 5 + 1) = 52√ó52\n",
    "3. **After pool1 (2√ó2, stride=2)**: 52/2 = 26√ó26\n",
    "4. **After conv2 (5√ó5, no padding)**: (26 - 5 + 1) = 22√ó22\n",
    "5. **After pool2 (2√ó2, stride=2)**: 22/2 = **11√ó11**\n",
    "\n",
    "With `num_channel*2` output channels from conv2, the flattened size is: `num_channel*2 √ó 11 √ó 11`\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Examine the number of parameters per layer. What do you notice?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "\n",
    "\n",
    "**Parameters per layer observations:**\n",
    "\n",
    "Looking at the torchinfo summary, you should notice:\n",
    "- **Convolutional layers have relatively few parameters**: A 5√ó5 conv with C_in=1, C_out=8 has only 8√ó(5√ó5 + 1) = 208 params\n",
    "- **First fully-connected layer has MANY parameters**: num_channel*2 * 11 * 11 * num_channel*4 is huge (e.g., 61,984 params with num_channel=8)\n",
    "- **FC layers dominate the parameter count**\n",
    "- **This is why modern CNNs minimize FC layers**: Using techniques like Global Average Pooling instead\n",
    "\n",
    "**Key insight:** The spatial dimensions (11√ó11) multiply the parameter count dramatically in FC layers, which is a major inefficiency compared to convolutional layers that share weights spatially.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-26e0c1522d99aab7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Training\n",
    "\n",
    "We define loss function and optimizer. Since we are modelling a classification problem we use the _cross-entropy loss_. The AdamW-Optimizer is a good default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-aa418fcc5649d483",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-39475ff98a1f8585",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Let's define the training-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    net: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: Callable,\n",
    "    device: str = \"cpu\",\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    net = net.to(device)\n",
    "\n",
    "    with tqdm(data_loader, unit=\"batch\", disable=not verbose) as tepoch:\n",
    "        total_samples_seen = 0\n",
    "        total_correct = 0\n",
    "\n",
    "        for _, (X, y) in enumerate(tepoch):\n",
    "            # Update Step\n",
    "            logits = net(X.to(device))\n",
    "            loss = loss_fn(logits, y.to(device))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate Accuracy\n",
    "            class_probabilities = torch.softmax(logits, axis=-1).detach().cpu()\n",
    "            y_hat = (\n",
    "                class_probabilities.argmax(dim=1, keepdim=True).squeeze().detach().cpu()\n",
    "            )\n",
    "\n",
    "            num_correct = (y_hat == y).sum().item()\n",
    "            num_samples = X.shape[0]\n",
    "            batch_accuracy = num_correct / num_samples\n",
    "\n",
    "            # Epoch Statistics\n",
    "            total_samples_seen += num_samples\n",
    "            total_correct += num_correct\n",
    "            epoch_accuracy = total_correct / total_samples_seen\n",
    "\n",
    "            if verbose:\n",
    "                tepoch.set_postfix(\n",
    "                    loss=loss.item(),\n",
    "                    accuracy_batch=batch_accuracy,\n",
    "                    accuracy_epoch=epoch_accuracy,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_mnist_train = torch.utils.data.DataLoader(\n",
    "    ds_mnist_train, batch_size=256, shuffle=True, num_workers=4\n",
    ")\n",
    "\n",
    "total_epochs = 5\n",
    "for epoch in range(0, total_epochs):\n",
    "    print(f\"Starting Epoch: {epoch + 1} / {total_epochs}\")\n",
    "    train_one_epoch(dl_mnist_train, net, optimizer, loss_fn, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate our model on test data. Lets define the test dataset and look at a few samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_mnist_test_tl = torch.utils.data.DataLoader(\n",
    "    ds_mnist_test_tl, batch_size=32, shuffle=False\n",
    ")\n",
    "\n",
    "images, labels = next(iter(dl_mnist_test_tl))\n",
    "\n",
    "ts.show(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Do you think the model will perform well?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "Tthe model should perform well on this test set.\n",
    "\n",
    "**Reasoning:**\n",
    "- **Training data**: The model was trained with `RandomQuadrantPad()` using a fixed quadrant (top-left).\n",
    "- **Test data**: The `ds_mnist_test_tl` dataset has digits also only in the top-left quadrant.\n",
    "- **Training covered this case**: Since training included digits in the top-left quadrant, the model has seen this configuration before\n",
    "\n",
    "**Expected outcome:**\n",
    "- **High accuracy** (likely >95%), the model should recognize digits in the top-left corner\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    net: torch.nn.Module,\n",
    "    loss_fn: Callable,\n",
    "    device: str = \"cpu\",\n",
    ") -> tuple[float, torch.Tensor, torch.Tensor]:\n",
    "    net = net.to(device)\n",
    "    net.eval()\n",
    "    with tqdm(data_loader, unit=\"batch\") as tepoch:\n",
    "        total_samples_seen = 0\n",
    "        total_correct = 0\n",
    "\n",
    "        y_list = []\n",
    "        y_hat_list = []\n",
    "\n",
    "        for _, (X, y) in enumerate(tepoch):\n",
    "            # Forward Pass\n",
    "            with torch.no_grad():\n",
    "                logits = net(X.to(device))\n",
    "            loss = loss_fn(logits, y.to(device))\n",
    "\n",
    "            # Predictions\n",
    "            class_probabilities = torch.softmax(logits, axis=-1).detach().cpu()\n",
    "            y_hat = (\n",
    "                class_probabilities.argmax(dim=1, keepdim=True).squeeze().detach().cpu()\n",
    "            )\n",
    "\n",
    "            # Metrics\n",
    "            num_correct = (y_hat == y).sum().item()\n",
    "            num_samples = X.shape[0]\n",
    "            total_samples_seen += num_samples\n",
    "            total_correct += num_correct\n",
    "            epoch_accuracy = total_correct / total_samples_seen\n",
    "\n",
    "            tepoch.set_postfix(\n",
    "                loss=loss.item(),\n",
    "                accuracy_epoch=epoch_accuracy,\n",
    "            )\n",
    "\n",
    "            # save preds and targets\n",
    "            y_list.append(y.cpu())\n",
    "            y_hat_list.append(y_hat.cpu())\n",
    "\n",
    "    return epoch_accuracy, torch.concat(y_list), torch.concat(y_hat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy, y, y_hat = eval_loop(dl_mnist_test_tl, net, loss_fn, device=device)\n",
    "\n",
    "print(f\"Test Accuracy:  {test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the following test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_mnist_test_br = torch.utils.data.DataLoader(\n",
    "    ds_mnist_test_br, batch_size=32, shuffle=False\n",
    ")\n",
    "\n",
    "images, labels = next(iter(dl_mnist_test_br))\n",
    "\n",
    "ts.show(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f7485417fbb9276a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: How good do you think the model works for this case?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**One possible hypothesis could be:**\n",
    "\n",
    "- Test data has digits only in the *bottom-right quadrant*.\n",
    "- The CNN was trained with digits only in the top-left quadrant, however, the same convolutional filters that detect edges, curves, and digit features in the top-left also detect them in the bottom-right.\n",
    "\n",
    "**Expected results:**: It might work! Similar accuracy to top-left test set** (likely >95%)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate your hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy, y, y_hat = eval_loop(dl_mnist_test_br, net, loss_fn, device=device)\n",
    "\n",
    "print(f\"Test Accuracy:  {test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What happened?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Oooopsy!!**: That does not look very well.\n",
    "\n",
    "\n",
    "**The issue is the fully connected layers**: The fully connected layers are not translation equivariant and thus have likely weights near zero for each neuron connected to quadrants without digits seen during trainig.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Try to improve the model by making architectural changes.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_channel=8, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Feature extractor ---\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv block 1\n",
    "            nn.Conv2d(1, num_channel, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Conv block 2\n",
    "            nn.Conv2d(num_channel, num_channel * 2, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        # --- Classifier ---\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(output_size=(1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_channel * 2, num_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_channel, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How easy is it?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "It is possible that boundary effects affect model performance -> digits on the top-left corner might not lead to the same activations as those at the bottom-right. In particular max-pooling is not perfectly invariant to translations.  You might want to replace them. An other option is to use data augmentation, e.g. randomly padding the border during model training.\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
